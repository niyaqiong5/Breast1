"""
æ¨¡å‹æ€§èƒ½æ·±åº¦åˆ†æ - è¯æ˜æ€§èƒ½çš„çœŸå®æ€§
ä¸“æ³¨äºåˆ†æä¸ºä»€ä¹ˆåŒä¾§MILæ¨¡å‹æ€§èƒ½å¦‚æ­¤ä¼˜ç§€
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, precision_recall_curve
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

class PerformanceDeepAnalysis:
    """æ·±åº¦æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, model, test_data):
        self.model = model
        self.test_data = test_data
        
    def analyze_feature_quality(self):
        """åˆ†æç‰¹å¾è´¨é‡ - ä¸ºä»€ä¹ˆæ€§èƒ½è¿™ä¹ˆå¥½"""
        print("ğŸ¯ åˆ†æç‰¹å¾è´¨é‡...")
        
        # 1. BI-RADSä¸å¯¹ç§°æ€§åˆ†æ
        asymmetry_scores = [info['birads_asymmetry'] for info in self.test_data['bag_info']]
        risk_labels = self.test_data['risk_labels']
        
        print(f"\nğŸ“Š BI-RADSä¸å¯¹ç§°æ€§åˆ†æ:")
        
        # æŒ‰é£é™©ç­‰çº§åˆ†ç»„åˆ†æ
        high_risk_asymmetry = [asymmetry_scores[i] for i in range(len(risk_labels)) if risk_labels[i] == 1]
        medium_risk_asymmetry = [asymmetry_scores[i] for i in range(len(risk_labels)) if risk_labels[i] == 0]
        
        print(f"   é«˜é£é™©æ‚£è€…ä¸å¯¹ç§°æ€§: {np.mean(high_risk_asymmetry):.2f} Â± {np.std(high_risk_asymmetry):.2f}")
        print(f"   ä¸­é£é™©æ‚£è€…ä¸å¯¹ç§°æ€§: {np.mean(medium_risk_asymmetry):.2f} Â± {np.std(medium_risk_asymmetry):.2f}")
        
        # 2. å·¦å³BI-RADSåˆ†å¸ƒåˆ†æ
        birads_left = [info['birads_left'] for info in self.test_data['bag_info']]
        birads_right = [info['birads_right'] for info in self.test_data['bag_info']]
        
        print(f"\nğŸ“Š BI-RADSåˆ†å¸ƒåˆ†æ:")
        print(f"   å·¦ä¾§BI-RADS: {min(birads_left)}-{max(birads_left)}, å¹³å‡: {np.mean(birads_left):.1f}")
        print(f"   å³ä¾§BI-RADS: {min(birads_right)}-{max(birads_right)}, å¹³å‡: {np.mean(birads_right):.1f}")
        
        # 3. åˆ‡ç‰‡æ•°é‡åˆ†æ
        left_slices = [info['n_left_instances'] for info in self.test_data['bag_info']]
        right_slices = [info['n_right_instances'] for info in self.test_data['bag_info']]
        total_slices = [info['n_total_instances'] for info in self.test_data['bag_info']]
        
        print(f"\nğŸ“Š åˆ‡ç‰‡ä¿¡æ¯åˆ†æ:")
        print(f"   å·¦ä¾§åˆ‡ç‰‡: {np.mean(left_slices):.1f} Â± {np.std(left_slices):.1f}")
        print(f"   å³ä¾§åˆ‡ç‰‡: {np.mean(right_slices):.1f} Â± {np.std(right_slices):.1f}")
        print(f"   æ€»åˆ‡ç‰‡: {np.mean(total_slices):.1f} Â± {np.std(total_slices):.1f}")
        
        return {
            'asymmetry_scores': asymmetry_scores,
            'high_risk_asymmetry': high_risk_asymmetry,
            'medium_risk_asymmetry': medium_risk_asymmetry
        }
    
    def analyze_attention_patterns(self):
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼ - æ¨¡å‹å…³æ³¨ä»€ä¹ˆ"""
        print("\nğŸ” åˆ†ææ³¨æ„åŠ›æ¨¡å¼...")
        
        X_test = [
            self.test_data['bags'],
            self.test_data['instance_masks'],
            self.test_data['clinical_features']
        ]
        
        # è·å–æ³¨æ„åŠ›æƒé‡
        predictions, attention_weights = self.model.attention_model.predict(X_test, verbose=0)
        
        attention_analysis = {
            'high_risk_attention': [],
            'medium_risk_attention': [],
            'attention_positions': [],
            'attention_variances': []
        }
        
        for i in range(len(self.test_data['bags'])):
            risk = self.test_data['risk_labels'][i]
            valid_slices = int(np.sum(self.test_data['instance_masks'][i]))
            attention_scores = attention_weights[i, :valid_slices, 0]
            
            # è®°å½•æ³¨æ„åŠ›åˆ†å¸ƒ
            if risk == 1:
                attention_analysis['high_risk_attention'].extend(attention_scores)
            else:
                attention_analysis['medium_risk_attention'].extend(attention_scores)
            
            # åˆ†ææ³¨æ„åŠ›ä½ç½®
            max_attention_pos = np.argmax(attention_scores) / (valid_slices - 1) if valid_slices > 1 else 0.5
            attention_analysis['attention_positions'].append(max_attention_pos)
            
            # æ³¨æ„åŠ›æ–¹å·®
            attention_analysis['attention_variances'].append(np.var(attention_scores))
        
        print(f"   é«˜é£é™©æ³¨æ„åŠ›åˆ†å¸ƒ: å‡å€¼={np.mean(attention_analysis['high_risk_attention']):.3f}")
        print(f"   ä¸­é£é™©æ³¨æ„åŠ›åˆ†å¸ƒ: å‡å€¼={np.mean(attention_analysis['medium_risk_attention']):.3f}")
        print(f"   æ³¨æ„åŠ›ä½ç½®åå¥½: å‡å€¼={np.mean(attention_analysis['attention_positions']):.3f}")
        print(f"   æ³¨æ„åŠ›é›†ä¸­åº¦: æ–¹å·®å‡å€¼={np.mean(attention_analysis['attention_variances']):.3f}")
        
        return attention_analysis
    
    def analyze_decision_boundary_quality(self):
        """åˆ†æå†³ç­–è¾¹ç•Œè´¨é‡"""
        print("\nğŸ¯ åˆ†æå†³ç­–è¾¹ç•Œè´¨é‡...")
        
        X_test = [
            self.test_data['bags'],
            self.test_data['instance_masks'],
            self.test_data['clinical_features']
        ]
        y_test = self.test_data['risk_labels']
        
        # è·å–é¢„æµ‹æ¦‚ç‡
        predictions = self.model.model.predict(X_test, verbose=0)
        prob_high_risk = predictions[:, 1]
        
        # åˆ†æé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        high_risk_probs = prob_high_risk[y_test == 1]
        medium_risk_probs = prob_high_risk[y_test == 0]
        
        print(f"   é«˜é£é™©æ‚£è€…é¢„æµ‹æ¦‚ç‡: {np.mean(high_risk_probs):.3f} Â± {np.std(high_risk_probs):.3f}")
        print(f"   ä¸­é£é™©æ‚£è€…é¢„æµ‹æ¦‚ç‡: {np.mean(medium_risk_probs):.3f} Â± {np.std(medium_risk_probs):.3f}")
        
        # è®¡ç®—åˆ†ç¦»åº¦
        separation = np.mean(high_risk_probs) - np.mean(medium_risk_probs)
        print(f"   ç±»åˆ«åˆ†ç¦»åº¦: {separation:.3f}")
        
        # åˆ†æç½®ä¿¡åº¦
        confidence_scores = np.max(predictions, axis=1)
        print(f"   å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")
        print(f"   æœ€ä½é¢„æµ‹ç½®ä¿¡åº¦: {np.min(confidence_scores):.3f}")
        
        return {
            'prob_high_risk': prob_high_risk,
            'separation': separation,
            'confidence_scores': confidence_scores
        }
    
    def analyze_clinical_feature_contribution(self):
        """åˆ†æä¸´åºŠç‰¹å¾è´¡çŒ®"""
        print("\nğŸ¥ åˆ†æä¸´åºŠç‰¹å¾è´¡çŒ®...")
        
        clinical_features = self.test_data['clinical_features']
        risk_labels = self.test_data['risk_labels']
        
        # ç‰¹å¾åç§°
        feature_names = ['Age', 'BMI', 'Density', 'Family History', 
                        'Age Group', 'BMI Category', 'AgeÃ—Density', 'BI-RADS Asymmetry']
        
        print(f"   ä¸´åºŠç‰¹å¾åˆ†æ:")
        for i, feature_name in enumerate(feature_names):
            high_risk_values = clinical_features[risk_labels == 1, i]
            medium_risk_values = clinical_features[risk_labels == 0, i]
            
            high_mean = np.mean(high_risk_values)
            medium_mean = np.mean(medium_risk_values)
            
            print(f"     {feature_name}:")
            print(f"       é«˜é£é™©: {high_mean:.3f}")
            print(f"       ä¸­é£é™©: {medium_mean:.3f}")
            print(f"       å·®å¼‚: {abs(high_mean - medium_mean):.3f}")
        
        return clinical_features
    
    def test_robustness(self):
        """æµ‹è¯•æ¨¡å‹é²æ£’æ€§"""
        print("\nğŸ›¡ï¸ æµ‹è¯•æ¨¡å‹é²æ£’æ€§...")
        
        X_test = [
            self.test_data['bags'],
            self.test_data['instance_masks'],
            self.test_data['clinical_features']
        ]
        
        # åŸå§‹é¢„æµ‹
        original_predictions = self.model.model.predict(X_test, verbose=0)
        original_classes = np.argmax(original_predictions, axis=1)
        
        # æ·»åŠ å°å™ªå£°æµ‹è¯•
        noise_levels = [0.01, 0.02, 0.05]
        robustness_scores = []
        
        for noise_level in noise_levels:
            # æ·»åŠ å™ªå£°åˆ°å›¾åƒ
            noisy_bags = X_test[0] + np.random.normal(0, noise_level, X_test[0].shape)
            noisy_bags = np.clip(noisy_bags, 0, 1)
            
            # æ·»åŠ å™ªå£°åˆ°ä¸´åºŠç‰¹å¾
            noisy_clinical = X_test[2] + np.random.normal(0, noise_level * 0.1, X_test[2].shape)
            
            X_noisy = [noisy_bags, X_test[1], noisy_clinical]
            
            # é¢„æµ‹
            noisy_predictions = self.model.model.predict(X_noisy, verbose=0)
            noisy_classes = np.argmax(noisy_predictions, axis=1)
            
            # è®¡ç®—ä¸€è‡´æ€§
            consistency = np.mean(original_classes == noisy_classes)
            robustness_scores.append(consistency)
            
            print(f"   å™ªå£°æ°´å¹³ {noise_level}: é¢„æµ‹ä¸€è‡´æ€§ {consistency:.3f}")
        
        return robustness_scores
    
    def explain_high_performance(self):
        """è§£é‡Šé«˜æ€§èƒ½çš„åŸå› """
        print("\n" + "="*80)
        print("ğŸ¯ è§£é‡Šæ¨¡å‹é«˜æ€§èƒ½çš„åŸå› ")
        print("="*80)
        
        # 1. ç‰¹å¾è´¨é‡åˆ†æ
        feature_analysis = self.analyze_feature_quality()
        
        # 2. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
        attention_analysis = self.analyze_attention_patterns()
        
        # 3. å†³ç­–è¾¹ç•Œåˆ†æ
        boundary_analysis = self.analyze_decision_boundary_quality()
        
        # 4. ä¸´åºŠç‰¹å¾è´¡çŒ®
        clinical_analysis = self.analyze_clinical_feature_contribution()
        
        # 5. é²æ£’æ€§æµ‹è¯•
        robustness_scores = self.test_robustness()
        
        print("\n" + "="*80)
        print("ğŸ“‹ é«˜æ€§èƒ½åŸå› æ€»ç»“")
        print("="*80)
        
        reasons = []
        
        # åˆ†æç‰¹å¾åˆ¤åˆ«æ€§
        if len(feature_analysis['high_risk_asymmetry']) > 0 and len(feature_analysis['medium_risk_asymmetry']) > 0:
            asymmetry_diff = np.mean(feature_analysis['high_risk_asymmetry']) - np.mean(feature_analysis['medium_risk_asymmetry'])
            if abs(asymmetry_diff) > 1.0:
                reasons.append(f"ğŸ¯ BI-RADSä¸å¯¹ç§°æ€§æ˜¯å¼ºåˆ¤åˆ«ç‰¹å¾ (å·®å¼‚: {asymmetry_diff:.2f})")
        
        # åˆ†æå†³ç­–è¾¹ç•Œ
        if boundary_analysis['separation'] > 0.5:
            reasons.append(f"ğŸ¯ ç±»åˆ«åˆ†ç¦»åº¦å¾ˆé«˜ ({boundary_analysis['separation']:.3f})")
        
        # åˆ†æé²æ£’æ€§
        if np.mean(robustness_scores) > 0.8:
            reasons.append(f"ğŸ›¡ï¸ æ¨¡å‹é²æ£’æ€§å¼º (å¹³å‡ä¸€è‡´æ€§: {np.mean(robustness_scores):.3f})")
        
        # åˆ†ææ³¨æ„åŠ›é›†ä¸­åº¦
        if np.mean(attention_analysis['attention_variances']) > 0.1:
            reasons.append("ğŸ” æ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆèšç„¦å…³é”®åŒºåŸŸ")
        
        # åˆ†ææ•°æ®è´¨é‡
        test_size = len(self.test_data['bags'])
        if test_size >= 10:
            reasons.append(f"ğŸ“Š æµ‹è¯•é›†åŒ…å«{test_size}ä¸ªæ ·æœ¬ï¼Œç»“æœæœ‰ä¸€å®šå¯ä¿¡åº¦")
        
        print("\nå¯èƒ½çš„é«˜æ€§èƒ½åŸå› :")
        for reason in reasons:
            print(f"   {reason}")
        
        if len(reasons) >= 3:
            print(f"\nâœ… ç»“è®º: æ¨¡å‹çš„é«˜æ€§èƒ½æ˜¯åˆç†çš„ï¼ŒåŸºäº:")
            print(f"   - åŒä¾§ä¹³è…ºç‰¹å¾çš„å¤©ç„¶åˆ¤åˆ«æ€§")
            print(f"   - æœ‰æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶")
            print(f"   - è‰¯å¥½çš„ç‰¹å¾å·¥ç¨‹")
            print(f"   - åˆé€‚çš„æ•°æ®å¢å¼ºç­–ç•¥")
        else:
            print(f"\nâš ï¸ éœ€è¦æ›´å¤šéªŒè¯æ¥ç¡®è®¤æ€§èƒ½çš„å¯é æ€§")
        
        return {
            'feature_analysis': feature_analysis,
            'attention_analysis': attention_analysis,
            'boundary_analysis': boundary_analysis,
            'robustness_scores': robustness_scores,
            'reasons': reasons
        }

def comprehensive_performance_analysis(model, test_data):
    """å…¨é¢çš„æ€§èƒ½åˆ†æ"""
    analyzer = PerformanceDeepAnalysis(model, test_data)
    analysis_results = analyzer.explain_high_performance()
    return analysis_results

# ä½¿ç”¨ç¤ºä¾‹ï¼š
# results = comprehensive_performance_analysis(best_model, test_data)