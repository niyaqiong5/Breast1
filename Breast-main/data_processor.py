"""
æ•°æ®å¤„ç†è„šæœ¬ - é€‚é…è®­ç»ƒæ–‡ä»¶
é›†æˆç°æœ‰çš„æ•°æ®å¤„ç†é€»è¾‘ï¼Œç”Ÿæˆè®­ç»ƒæ‰€éœ€çš„æ•°æ®æ ¼å¼
ä¿æŒä¸åŸä»£ç ç›¸åŒçš„æ•°æ®é¢„å¤„ç†é€»è¾‘
"""

import os
import sys
import argparse
import logging
import json
import pickle
import gzip
import h5py
import numpy as np
import pandas as pd
from tqdm import tqdm
import cv2
from skimage.transform import resize
from skimage.exposure import equalize_hist
from scipy.ndimage import binary_erosion, binary_dilation, center_of_mass, label, distance_transform_edt, gaussian_filter
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("data_processor.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# å¯¼å…¥ç°æœ‰æ¨¡å—
try:
    from data_loader import ClinicalDataLoader, DicomLoader, SegmentationLoader
    from image_features import BilateralImageProcessor
    logger.info("âœ… æˆåŠŸå¯¼å…¥ç°æœ‰æ•°æ®å¤„ç†æ¨¡å—")
except ImportError as e:
    logger.error(f"âŒ ç¼ºå°‘å¿…éœ€çš„æ¨¡å—: {e}")
    logger.error("è¯·ç¡®ä¿ data_loader.py å’Œ image_features.py æ–‡ä»¶åœ¨åŒä¸€ç›®å½•")
    sys.exit(1)

def safe_json_convert(obj):
    """å®‰å…¨çš„JSONè½¬æ¢å‡½æ•°ï¼Œå¤„ç†numpyå’Œpandasç±»å‹"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, pd.Series):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: safe_json_convert(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [safe_json_convert(item) for item in obj]
    else:
        return obj

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ - ä½¿ç”¨å›ºå®šç¼“å­˜åç§°"""
    
    def __init__(self, cache_root='./cache'):
        self.cache_root = cache_root
        self.cache_dir = os.path.join(cache_root, 'optimized_cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„ç¼“å­˜åç§°
        self.cache_name = "breast_data_v1"
        
        logger.info(f"ğŸ—„ï¸ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–")
        logger.info(f"   ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"   ç¼“å­˜åç§°: {self.cache_name}")
    
    def get_cache_files(self):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return {
            'clinical': os.path.join(self.cache_dir, f"{self.cache_name}_clinical.pkl.gz"),
            'images': os.path.join(self.cache_dir, f"{self.cache_name}_images.h5"),
            'mapping': os.path.join(self.cache_dir, f"{self.cache_name}_mapping.pkl.gz")
        }
    
    def cache_exists(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_files = self.get_cache_files()
        
        all_exist = all(os.path.exists(f) and os.path.getsize(f) > 0 for f in cache_files.values())
        
        if all_exist:
            total_size = sum(os.path.getsize(f) for f in cache_files.values()) / (1024*1024)
            logger.info(f"ğŸ¯ å‘ç°ç¼“å­˜!")
            logger.info(f"   æ€»å¤§å°: {total_size:.1f} MB")
            return True
        else:
            return False
    
    def save_cache(self, clinical_df, bilateral_image_features, processing_config):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        logger.info("ğŸ’¾ ä¿å­˜æ•°æ®åˆ°ç¼“å­˜...")
        
        cache_files = self.get_cache_files()
        
        try:
            # ä¿å­˜ä¸´åºŠæ•°æ®
            with gzip.open(cache_files['clinical'], 'wb') as f:
                pickle.dump(clinical_df, f)
            logger.info(f"âœ… ä¸´åºŠæ•°æ®å·²ä¿å­˜: {clinical_df.shape}")
            
            # ä¿å­˜å›¾åƒæ•°æ®
            with h5py.File(cache_files['images'], 'w') as hf:
                patient_count = len(bilateral_image_features)
                logger.info(f"ğŸ“Š ä¿å­˜ {patient_count} ä¸ªæ‚£è€…çš„å›¾åƒæ•°æ®...")
                
                for pid, data in tqdm(bilateral_image_features.items(), desc="ä¿å­˜å›¾åƒç¼“å­˜"):
                    patient_group = hf.create_group(pid)
                    
                    # ä¿å­˜å·¦å³ä¹³æˆ¿å›¾åƒ
                    if 'left_images' in data:
                        patient_group.create_dataset('left_images', data=data['left_images'])
                    if 'right_images' in data:
                        patient_group.create_dataset('right_images', data=data['right_images'])
            
            logger.info(f"âœ… å›¾åƒæ•°æ®å·²ä¿å­˜: {len(bilateral_image_features)} ä¸ªæ‚£è€…")
            
            # ä¿å­˜æ˜ å°„æ•°æ®
            mapping_data = {
                'processing_config': processing_config
            }
            
            with gzip.open(cache_files['mapping'], 'wb') as f:
                pickle.dump(mapping_data, f)
            logger.info(f"âœ… æ˜ å°„æ•°æ®å·²ä¿å­˜")
            
            total_size = sum(os.path.getsize(f) for f in cache_files.values()) / (1024*1024)
            logger.info(f"ğŸ‰ ç¼“å­˜ä¿å­˜å®Œæˆ! æ€»å¤§å°: {total_size:.1f} MB")
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            # æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´æ–‡ä»¶
            for file_path in cache_files.values():
                if os.path.exists(file_path):
                    os.remove(file_path)
            raise
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        logger.info("ğŸ“‚ ä»ç¼“å­˜åŠ è½½æ•°æ®...")
        
        cache_files = self.get_cache_files()
        
        try:
            # åŠ è½½ä¸´åºŠæ•°æ®
            with gzip.open(cache_files['clinical'], 'rb') as f:
                clinical_df = pickle.load(f)
            logger.info(f"âœ… ä¸´åºŠæ•°æ®å·²åŠ è½½: {clinical_df.shape}")
            
            # åŠ è½½å›¾åƒæ•°æ®
            bilateral_image_features = {}
            with h5py.File(cache_files['images'], 'r') as hf:
                patient_count = len(hf.keys())
                logger.info(f"ğŸ“Š åŠ è½½ {patient_count} ä¸ªæ‚£è€…çš„å›¾åƒæ•°æ®...")
                
                for pid in tqdm(hf.keys(), desc="åŠ è½½å›¾åƒç¼“å­˜"):
                    patient_group = hf[pid]
                    image_data = {}
                    
                    # åŠ è½½å·¦å³ä¹³æˆ¿å›¾åƒ
                    if 'left_images' in patient_group:
                        image_data['left_images'] = patient_group['left_images'][:]
                    if 'right_images' in patient_group:
                        image_data['right_images'] = patient_group['right_images'][:]
                    
                    bilateral_image_features[pid] = image_data
            
            logger.info(f"âœ… å›¾åƒæ•°æ®å·²åŠ è½½: {len(bilateral_image_features)} ä¸ªæ‚£è€…")
            
            # åŠ è½½æ˜ å°„æ•°æ®
            with gzip.open(cache_files['mapping'], 'rb') as f:
                mapping_data = pickle.load(f)
            logger.info(f"âœ… æ˜ å°„æ•°æ®å·²åŠ è½½")
            
            # é‡æ„æ•°æ®
            cached_data = {
                'clinical_features': clinical_df,
                'bilateral_image_features': bilateral_image_features,
                'processing_config': mapping_data.get('processing_config', {})
            }
            
            return cached_data
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            raise

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ - ä¿æŒåŸæœ‰é€»è¾‘"""
    
    def __init__(self, config):
        self.config = config
        self.cache_manager = CacheManager(config.get('cache_root', './cache'))
        
        # æ•°æ®è·¯å¾„
        self.data_paths = config['data_paths']
        
        # å›¾åƒå¤„ç†é…ç½®
        self.image_config = config['image_config']
        
        # è¾“å‡ºç›®å½•
        self.output_dir = config.get('output_dir', './processed_data')
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info("ğŸš€ æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_and_process_data(self, force_rebuild=False):
        """åŠ è½½å’Œå¤„ç†æ‰€æœ‰æ•°æ®"""
        
        print("=" * 80)
        print("ğŸ—„ï¸ æ•°æ®åŠ è½½å’Œå¤„ç†")
        print("=" * 80)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_manager.cache_exists() and not force_rebuild:
            logger.info("ğŸ¯ å‘ç°ç¼“å­˜! ç›´æ¥åŠ è½½...")
            try:
                cached_data = self.cache_manager.load_cache()
                
                print("=" * 80)
                print("ğŸ‰ æˆåŠŸ! ä»ç¼“å­˜åŠ è½½æ•°æ®")
                print(f"âœ… åŠ è½½ {len(cached_data['bilateral_image_features'])} ä¸ªæ‚£è€…çš„æ•°æ®")
                print("=" * 80)
                
                self._print_data_summary(cached_data)
                return cached_data
                
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
                logger.info("ğŸ”„ å°†é‡æ–°å¤„ç†æ•°æ®...")
        
        # é‡æ–°å¤„ç†æ•°æ®
        logger.info("ğŸ”„ å¼€å§‹å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹...")
        
        # 1. åŠ è½½ä¸´åºŠæ•°æ®
        logger.info("ğŸ“Š åŠ è½½ä¸´åºŠæ•°æ®...")
        clinical_loader = ClinicalDataLoader(self.data_paths['excel_path'])
        clinical_df = clinical_loader.load_data()
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_columns = ['PID', 'BI-RADSl', 'BI-RADSr', 'å¹´é¾„', 'BMI', 'density', 'history']
        missing_columns = [col for col in required_columns if col not in clinical_df.columns]
        if missing_columns:
            raise ValueError(f"ä¸´åºŠæ•°æ®ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        
        logger.info(f"ä¸´åºŠæ•°æ®åŠ è½½å®Œæˆ: {len(clinical_df)} æ¡è®°å½•")
        
        # 2. æ£€æŸ¥å’Œæ¸…ç†BI-RADSæ•°æ®
        clinical_df = self._check_and_clean_birads_data(clinical_df)
        
        # 3. åŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ®
        logger.info("ğŸ–¼ï¸ åŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ®...")
        bilateral_image_features = self._load_and_process_images(clinical_df)
        
        # 4. ä¿å­˜åˆ°ç¼“å­˜
        self.cache_manager.save_cache(clinical_df, bilateral_image_features, self.image_config)
        
        # 5. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
        self._generate_data_quality_report(clinical_df, bilateral_image_features)
        
        cached_data = {
            'clinical_features': clinical_df,
            'bilateral_image_features': bilateral_image_features,
            'processing_config': self.image_config
        }
        
        print("=" * 80)
        print("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆå¹¶ä¿å­˜åˆ°ç¼“å­˜")
        print(f"âœ… å¤„ç† {len(bilateral_image_features)} ä¸ªæ‚£è€…çš„æ•°æ®")
        print("=" * 80)
        
        self._print_data_summary(cached_data)
        
        return cached_data
    
    def _check_and_clean_birads_data(self, clinical_df):
        """æ£€æŸ¥å’Œæ¸…ç†BI-RADSæ•°æ® - ä¿æŒåŸæœ‰é€»è¾‘"""
        
        logger.info("ğŸ” æ£€æŸ¥BI-RADSæ•°æ®è´¨é‡...")
        
        original_count = len(clinical_df)
        
        # æ£€æŸ¥BI-RADSå€¼çš„æœ‰æ•ˆæ€§
        valid_birads = [1, 2, 3, 4, 5, 6]
        
        # è¿‡æ»¤æ— æ•ˆçš„BI-RADSå€¼
        valid_mask = (
            clinical_df['BI-RADSl'].isin(valid_birads) & 
            clinical_df['BI-RADSr'].isin(valid_birads)
        )
        
        clinical_df = clinical_df[valid_mask].copy()
        
        filtered_count = len(clinical_df)
        removed_count = original_count - filtered_count
        
        logger.info(f"BI-RADSæ•°æ®æ£€æŸ¥å®Œæˆ:")
        logger.info(f"  åŸå§‹è®°å½•: {original_count}")
        logger.info(f"  æœ‰æ•ˆè®°å½•: {filtered_count}")
        logger.info(f"  ç§»é™¤è®°å½•: {removed_count}")
        
        # ç»Ÿè®¡BI-RADSåˆ†å¸ƒ
        logger.info("BI-RADSåˆ†å¸ƒç»Ÿè®¡:")
        logger.info(f"  å·¦ä¹³æˆ¿: {clinical_df['BI-RADSl'].value_counts().sort_index().to_dict()}")
        logger.info(f"  å³ä¹³æˆ¿: {clinical_df['BI-RADSr'].value_counts().sort_index().to_dict()}")
        
        return clinical_df
    
    def _load_and_process_images(self, clinical_df):
        """åŠ è½½å’Œå¤„ç†å›¾åƒæ•°æ® - ä¿æŒåŸæœ‰é€»è¾‘"""
        
        # åˆ›å»ºå›¾åƒå¤„ç†å™¨ - ä¿æŒåŸæœ‰é…ç½®
        image_config = self.image_config.copy()
        image_config['bilateral_mode'] = 'separate_channels'  # ç¡®ä¿åˆ†ç¦»æ¨¡å¼
        
        bilateral_processor = BilateralImageProcessor(image_config)
        
        # åŠ è½½DICOMå’Œåˆ†å‰²æ•°æ®
        dicom_loader = DicomLoader(self.data_paths['dicom_root_dir'])
        segmentation_loader = SegmentationLoader(self.data_paths['segmentation_root_dir'])
        
        # æ”¶é›†æ‚£è€…æ•°æ®
        patient_data = {}
        successful_patients = 0
        
        unique_pids = clinical_df['PID'].unique()
        logger.info(f"å¼€å§‹å¤„ç† {len(unique_pids)} ä¸ªæ‚£è€…çš„å›¾åƒæ•°æ®...")
        
        for pid in tqdm(unique_pids, desc="åŠ è½½æ‚£è€…å›¾åƒæ•°æ®"):
            try:
                dicom_data = dicom_loader.load_patient_dicom(pid)
                segmentation_data = segmentation_loader.load_patient_segmentation(pid)
                
                if dicom_data is not None and segmentation_data is not None:
                    patient_data[pid] = (dicom_data, segmentation_data)
                    successful_patients += 1
                    
                    # å†…å­˜ç®¡ç†
                    if successful_patients % 10 == 0:
                        import gc
                        gc.collect()
                        
            except Exception as e:
                logger.warning(f"æ‚£è€… {pid} å›¾åƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸåŠ è½½ {successful_patients} ä¸ªæ‚£è€…çš„å›¾åƒæ•°æ®")
        
        if successful_patients == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ‚£è€…çš„å›¾åƒæ•°æ®")
        
        # æ‰¹é‡å¤„ç†å›¾åƒ - ä¿æŒåŸæœ‰é€»è¾‘
        logger.info("ğŸ”„ æ‰¹é‡å¤„ç†åŒä¾§å›¾åƒ...")
        processed_bilateral_data = bilateral_processor.batch_process_bilateral(patient_data)
        
        # æå–2Dåˆ‡ç‰‡ - ä¿æŒåŸæœ‰é€»è¾‘
        bilateral_slices_data = bilateral_processor.extract_bilateral_2d_slices(processed_bilateral_data)
        
        # æ„å»ºå›¾åƒç‰¹å¾å­—å…¸ - ä¿æŒåŸæœ‰æ ¼å¼
        bilateral_image_features = {}
        for i, pid in enumerate(bilateral_slices_data['slice_to_patient']):
            if pid not in bilateral_image_features:
                bilateral_image_features[pid] = {
                    'bilateral_images': [],
                    'left_images': [],
                    'right_images': []
                }
            bilateral_image_features[pid]['bilateral_images'].append(bilateral_slices_data['bilateral_slices'][i])
            bilateral_image_features[pid]['left_images'].append(bilateral_slices_data['left_slices'][i])
            bilateral_image_features[pid]['right_images'].append(bilateral_slices_data['right_slices'][i])
        
        logger.info(f"å›¾åƒå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(bilateral_image_features)} ä¸ªæ‚£è€…")
        
        return bilateral_image_features
    
    def _generate_data_quality_report(self, clinical_df, bilateral_image_features):
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        
        logger.info("ğŸ“‹ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
        
        report = {
            'clinical_data': {
                'total_patients': int(len(clinical_df)),
                'age_range': [float(clinical_df['å¹´é¾„'].min()), float(clinical_df['å¹´é¾„'].max())],
                'age_mean': float(clinical_df['å¹´é¾„'].mean()),
                'bmi_range': [float(clinical_df['BMI'].min()), float(clinical_df['BMI'].max())],
                'bmi_mean': float(clinical_df['BMI'].mean()),
                'density_distribution': {k: int(v) for k, v in clinical_df['density'].value_counts().to_dict().items()},
                'history_distribution': {k: int(v) for k, v in clinical_df['history'].value_counts().to_dict().items()},
                'birads_left_distribution': {int(k): int(v) for k, v in clinical_df['BI-RADSl'].value_counts().sort_index().to_dict().items()},
                'birads_right_distribution': {int(k): int(v) for k, v in clinical_df['BI-RADSr'].value_counts().sort_index().to_dict().items()}
            },
            'image_data': {
                'patients_with_images': int(len(bilateral_image_features)),
                'total_slices': int(sum(len(data['bilateral_images']) for data in bilateral_image_features.values()))
            }
        }
        
        # åŒ¹é…ç»Ÿè®¡
        clinical_pids = set(clinical_df['PID'].astype(str))
        image_pids = set(bilateral_image_features.keys())
        common_pids = clinical_pids.intersection(image_pids)
        
        report['data_matching'] = {
            'clinical_only': int(len(clinical_pids - image_pids)),
            'image_only': int(len(image_pids - clinical_pids)),
            'both_available': int(len(common_pids)),
            'coverage_rate': float(len(common_pids) / len(clinical_pids) if clinical_pids else 0)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, 'data_quality_report.json')
        
        # ä½¿ç”¨safe_json_convertç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½åºåˆ—åŒ–
        safe_report = safe_json_convert(report)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(safe_report, f, indent=4, ensure_ascii=False)
        
        logger.info(f"æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        logger.info(f"æ•°æ®è¦†ç›–ç‡: {report['data_matching']['coverage_rate']:.2%}")
    
    def _print_data_summary(self, data):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        clinical_df = data['clinical_features']
        bilateral_image_features = data['bilateral_image_features']
        
        logger.info("ğŸ“Š æ•°æ®æ‘˜è¦:")
        logger.info(f"   ä¸´åºŠæ•°æ®: {len(clinical_df)} ä¸ªæ‚£è€…")
        logger.info(f"   å›¾åƒæ•°æ®: {len(bilateral_image_features)} ä¸ªæ‚£è€…")
        
        # BI-RADSåˆ†å¸ƒ
        logger.info("   BI-RADSåˆ†å¸ƒ:")
        for side, col in [('å·¦ä¹³æˆ¿', 'BI-RADSl'), ('å³ä¹³æˆ¿', 'BI-RADSr')]:
            dist = clinical_df[col].value_counts().sort_index()
            logger.info(f"     {side}: {dist.to_dict()}")
        
        # å›¾åƒåˆ‡ç‰‡ç»Ÿè®¡
        total_slices = sum(len(data['bilateral_images']) for data in bilateral_image_features.values())
        avg_slices = total_slices / len(bilateral_image_features) if bilateral_image_features else 0
        logger.info(f"   æ€»åˆ‡ç‰‡æ•°: {total_slices}")
        logger.info(f"   å¹³å‡åˆ‡ç‰‡æ•°: {avg_slices:.1f}")

def create_training_data_format(processed_data, output_dir):
    """åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼"""
    
    logger.info("ğŸ“ åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼...")
    
    clinical_df = processed_data['clinical_features']
    bilateral_image_features = processed_data['bilateral_image_features']
    
    # åŒ¹é…ä¸´åºŠå’Œå›¾åƒæ•°æ®
    clinical_pids = set(clinical_df['PID'].astype(str))
    image_pids = set(bilateral_image_features.keys())
    common_pids = list(clinical_pids.intersection(image_pids))
    
    logger.info(f"å…±åŒæ‚£è€…æ•°: {len(common_pids)}")
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼çš„è¯´æ˜æ–‡æ¡£
    readme_content = f"""
# è®­ç»ƒæ•°æ®æ ¼å¼è¯´æ˜

## æ•°æ®ç»“æ„
- ä¸´åºŠæ•°æ®: {len(clinical_df)} ä¸ªæ‚£è€…
- å›¾åƒæ•°æ®: {len(bilateral_image_features)} ä¸ªæ‚£è€…  
- åŒ¹é…æ‚£è€…: {len(common_pids)} ä¸ªæ‚£è€…

## æ•°æ®è®¿é—®æ–¹å¼

### 1. ä¸´åºŠæ•°æ® (clinical_features)
```python
clinical_df = processed_data['clinical_features']
# åŒ…å«åˆ—: PID, BI-RADSl, BI-RADSr, å¹´é¾„, BMI, density, history ç­‰
```

### 2. å›¾åƒæ•°æ® (bilateral_image_features)
```python
bilateral_image_features = processed_data['bilateral_image_features']
# å­—å…¸æ ¼å¼: {{patient_id: {{'left_images': [...], 'right_images': [...], 'bilateral_images': [...]}}}}
```

### 3. ä½¿ç”¨ç¤ºä¾‹
```python
# éå†æ‰€æœ‰æ‚£è€…
for pid in common_pids:
    # è·å–ä¸´åºŠæ•°æ®
    patient_clinical = clinical_df[clinical_df['PID'].astype(str) == pid].iloc[0]
    
    # è·å–å›¾åƒæ•°æ®
    patient_images = bilateral_image_features[pid]
    left_images = patient_images['left_images']      # å·¦ä¹³æˆ¿å›¾åƒåˆ—è¡¨
    right_images = patient_images['right_images']    # å³ä¹³æˆ¿å›¾åƒåˆ—è¡¨
    bilateral_images = patient_images['bilateral_images']  # åŒä¾§å›¾åƒåˆ—è¡¨
    
    # è·å–æ ‡ç­¾
    birads_left = patient_clinical['BI-RADSl']
    birads_right = patient_clinical['BI-RADSr']
```

## æ•°æ®é¢„å¤„ç†ç‰¹ç‚¹
- å›¾åƒå·²ç»æ ‡å‡†åŒ–åˆ°æŒ‡å®šå°ºå¯¸
- å·²ç»è¿›è¡Œäº†åŒä¾§åˆ†ç¦»
- ä¿æŒäº†åŸæœ‰çš„é¢„å¤„ç†é€»è¾‘
- ä¸´åºŠç‰¹å¾å·²ç»æ¸…ç†å’ŒéªŒè¯

## é£é™©åˆ†å±‚
- BI-RADS 1-3: ä¸­ä½é£é™©
- BI-RADS 4-6: é«˜é£é™©

ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}
"""
    
    # ä¿å­˜è¯´æ˜æ–‡æ¡£
    readme_path = os.path.join(output_dir, 'README.md')
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    # åˆ›å»ºæ•°æ®è®¿é—®ç¤ºä¾‹
    example_code = f"""
# æ•°æ®è®¿é—®ç¤ºä¾‹ä»£ç 
import pickle
import gzip

# åŠ è½½å¤„ç†åçš„æ•°æ®
def load_processed_data():
    # è¿™é‡Œåº”è¯¥ç”¨æ‚¨çš„å®é™…æ•°æ®åŠ è½½æ–¹æ³•
    # ä¾‹å¦‚ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨
    cache_manager = CacheManager('./cache')
    processed_data = cache_manager.load_cache()
    return processed_data

# ä½¿ç”¨ç¤ºä¾‹
processed_data = load_processed_data()
clinical_df = processed_data['clinical_features']
bilateral_image_features = processed_data['bilateral_image_features']

# åŒ¹é…æ‚£è€…
clinical_pids = set(clinical_df['PID'].astype(str))
image_pids = set(bilateral_image_features.keys())
common_pids = list(clinical_pids.intersection(image_pids))

print(f"å¯ç”¨äºè®­ç»ƒçš„æ‚£è€…æ•°: {{len(common_pids)}}")

# éå†æ‚£è€…è¿›è¡Œè®­ç»ƒæ•°æ®å‡†å¤‡
for pid in common_pids[:5]:  # ç¤ºä¾‹ï¼šå‰5ä¸ªæ‚£è€…
    # ä¸´åºŠæ•°æ®
    patient_row = clinical_df[clinical_df['PID'].astype(str) == pid].iloc[0]
    
    # å›¾åƒæ•°æ®
    image_data = bilateral_image_features[pid]
    
    print(f"æ‚£è€… {{pid}}:")
    print(f"  BI-RADSå·¦: {{patient_row['BI-RADSl']}}")
    print(f"  BI-RADSå³: {{patient_row['BI-RADSr']}}")
    print(f"  å·¦ä¹³æˆ¿å›¾åƒæ•°: {{len(image_data['left_images'])}}")
    print(f"  å³ä¹³æˆ¿å›¾åƒæ•°: {{len(image_data['right_images'])}}")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ‚¨çš„è®­ç»ƒæ•°æ®å‡†å¤‡é€»è¾‘
"""
    
    example_path = os.path.join(output_dir, 'data_access_example.py')
    with open(example_path, 'w', encoding='utf-8') as f:
        f.write(example_code)
    
    logger.info(f"âœ… è®­ç»ƒæ•°æ®æ ¼å¼æ–‡æ¡£å·²åˆ›å»º:")
    logger.info(f"   è¯´æ˜æ–‡æ¡£: {readme_path}")
    logger.info(f"   ç¤ºä¾‹ä»£ç : {example_path}")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    
    parser = argparse.ArgumentParser(description='æ•°æ®å¤„ç†è„šæœ¬ - é€‚é…è®­ç»ƒæ–‡ä»¶')
    parser.add_argument('--data_dir', type=str, default='D:/Desktop/', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='processed_data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--cache_root', type=str, default='./cache', help='ç¼“å­˜æ ¹ç›®å½•')
    parser.add_argument('--image_size', type=int, default=128, help='å›¾åƒå°ºå¯¸')
    parser.add_argument('--max_slices', type=int, default=20, help='æœ€å¤§åˆ‡ç‰‡æ•°')
    parser.add_argument('--force_rebuild', action='store_true', help='å¼ºåˆ¶é‡æ–°å¤„ç†')
    
    args = parser.parse_args()
    
    # é…ç½®è®¾ç½®
    config = {
        'output_dir': args.output_dir,
        'cache_root': args.cache_root,
        'data_paths': {
            'excel_path': os.path.join(args.data_dir, 'Breast BI-RADS.xlsx'),
            'dicom_root_dir': os.path.join(args.data_dir, 'Data_BI-RADS'),
            'segmentation_root_dir': os.path.join(args.data_dir, 'Data_BI-RADS_segmentation')
        },
        'image_config': {
            'target_size': (args.image_size, args.image_size),
            'max_slices': args.max_slices,
            'save_debug_images': False,
            'normalization': 'z_score',
            'bilateral_mode': 'separate_channels',
            'output_dir': os.path.join(args.output_dir, 'processed_images')
        }
    }
    
    logger.info("ğŸš€ å¼€å§‹æ•°æ®å¤„ç†")
    logger.info("="*80)
    logger.info(f"é…ç½®ä¿¡æ¯:")
    logger.info(f"  æ•°æ®ç›®å½•: {args.data_dir}")
    logger.info(f"  è¾“å‡ºç›®å½•: {config['output_dir']}")
    logger.info(f"  ç¼“å­˜ç›®å½•: {config['cache_root']}")
    logger.info(f"  å›¾åƒå°ºå¯¸: {config['image_config']['target_size']}")
    logger.info(f"  æœ€å¤§åˆ‡ç‰‡æ•°: {config['image_config']['max_slices']}")
    logger.info("="*80)
    
    try:
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        processor = DataProcessor(config)
        
        # åŠ è½½å’Œå¤„ç†æ•°æ®
        processed_data = processor.load_and_process_data(force_rebuild=args.force_rebuild)
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼æ–‡æ¡£
        create_training_data_format(processed_data, args.output_dir)
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ æ•°æ®å¤„ç†å®Œæˆ!")
        logger.info("="*80)
        logger.info("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        logger.info(f"   ç¼“å­˜ç›®å½•: {config['cache_root']}/optimized_cache/")
        logger.info(f"   è´¨é‡æŠ¥å‘Š: {args.output_dir}/data_quality_report.json")
        logger.info(f"   ä½¿ç”¨è¯´æ˜: {args.output_dir}/README.md")
        logger.info(f"   ç¤ºä¾‹ä»£ç : {args.output_dir}/data_access_example.py")
        logger.info("\nğŸ‰ æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºè®­ç»ƒ!")
        logger.info("è¯·åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨ CacheManager åŠ è½½å¤„ç†åçš„æ•°æ®")
        logger.info("="*80)
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)