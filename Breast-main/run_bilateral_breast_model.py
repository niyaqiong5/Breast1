"""
å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ä¹³è…ºç™Œé£é™©é¢„æµ‹æ¨¡å‹ - åŒä¾§æ•´ä½“è®­ç»ƒç‰ˆæœ¬
ä¿®æ­£ç‰ˆï¼šç§»é™¤è®­ç»ƒæ—¶çš„BI-RADSä¸å¯¹ç§°ç‰¹å¾ï¼Œè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ åŒä¾§ä¸å¯¹ç§°
"""

import os
import sys
import argparse
import logging
import tensorflow as tf
from datetime import datetime
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle
import json
import matplotlib.pyplot as plt
import gzip
import h5py
import cv2
import seaborn as sns

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Dropout, BatchNormalization, 
    LeakyReLU, Lambda, Multiply, Layer, Concatenate,
    GlobalAveragePooling2D, Subtract, Add
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LambdaCallback
from tensorflow.keras.regularizers import l1_l2
import tensorflow.keras.backend as K

# å¯¼å…¥ç°æœ‰æ¨¡å—
try:
    # å¯¼å…¥æ–°çš„å¯è§†åŒ–æ¨¡å—
    from bilateral_attention_viz import EnhancedBilateralAttentionVisualizer, visualize_bilateral_model_performance
    from bilateral_gradcam_viz import generate_true_attention_visualizations
    from bilateral_visualizations import ComprehensiveBilateralVisualization, run_comprehensive_visualization
    logger = logging.getLogger(__name__)
    from bilateral_gradcam_enhanced import generate_improved_bilateral_gradcam
    from bilateral_asymmetry_visualization import visualize_bilateral_asymmetry_learning
    logger.info("âœ… Successfully imported all data processing modules")
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.error(f"âŒ Missing required modules: {e}")

# ç¯å¢ƒè®¾ç½®
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'

# æ—¥å¿—è®¾ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"bilateral_mil_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def setup_gpu():
    """GPUè®¾ç½®"""
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info(f"âœ… GPU configured: {len(gpus)} GPU(s)")
        else:
            logger.info("â„¹ï¸ Using CPU")
    except Exception as e:
        logger.warning(f"âš ï¸ GPU configuration failed: {e}")

def safe_json_convert(obj):
    """å®‰å…¨çš„JSONè½¬æ¢"""
    try:
        if hasattr(obj, 'numpy'):
            return float(obj.numpy()) if obj.numpy().ndim == 0 else obj.numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: safe_json_convert(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [safe_json_convert(item) for item in obj]
        else:
            return obj
    except:
        return str(obj)

def format_confusion_matrix(cm, class_names=None):
    """æ ¼å¼åŒ–æ··æ·†çŸ©é˜µä¸ºå¯è¯»å­—ç¬¦ä¸²"""
    if class_names is None:
        class_names = ['Medium Risk', 'High Risk']
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ¼å¼åŒ–å­—ç¬¦ä¸²
    matrix_str = "\n      Predicted\n"
    matrix_str += "      Med  High\n"
    matrix_str += "True Med  {:2d}   {:2d}  ({:.1f}% | {:.1f}%)\n".format(
        cm[0,0], cm[0,1], cm_percent[0,0], cm_percent[0,1])
    matrix_str += "    High  {:2d}   {:2d}  ({:.1f}% | {:.1f}%)\n".format(
        cm[1,0], cm[1,1], cm_percent[1,0], cm_percent[1,1])
    
    return matrix_str

class FixedCacheManager:
    """ä½¿ç”¨å›ºå®šç¼“å­˜åç§°çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_root='./cache'):
        self.cache_root = cache_root
        self.cache_dir = os.path.join(cache_root, 'optimized_cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„ç¼“å­˜åç§°
        self.cache_name = "breast_data_v1"
        
        logger.info(f"ğŸ—„ï¸ Fixed cache manager initialized")
        logger.info(f"   Cache directory: {self.cache_dir}")
        logger.info(f"   Fixed cache name: {self.cache_name}")
    
    def get_cache_files(self):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return {
            'clinical': os.path.join(self.cache_dir, f"{self.cache_name}_clinical.pkl.gz"),
            'images': os.path.join(self.cache_dir, f"{self.cache_name}_images.h5"),
            'mapping': os.path.join(self.cache_dir, f"{self.cache_name}_mapping.pkl.gz")
        }
    
    def cache_exists(self):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_files = self.get_cache_files()
        
        all_exist = all(os.path.exists(f) and os.path.getsize(f) > 0 for f in cache_files.values())
        
        if all_exist:
            total_size = sum(os.path.getsize(f) for f in cache_files.values()) / (1024*1024)
            logger.info(f"ğŸ¯ CACHE FOUND!")
            logger.info(f"   Total size: {total_size:.1f} MB")
            return True
        else:
            return False
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        logger.info("ğŸ“‚ Loading data from fixed cache...")
        
        cache_files = self.get_cache_files()
        
        try:
            # åŠ è½½ä¸´åºŠæ•°æ®
            with gzip.open(cache_files['clinical'], 'rb') as f:
                clinical_df = pickle.load(f)
            logger.info(f"âœ… Clinical data loaded: {clinical_df.shape}")
            
            # åŠ è½½å›¾åƒæ•°æ®
            bilateral_image_features = {}
            with h5py.File(cache_files['images'], 'r') as hf:
                patient_count = len(hf.keys())
                logger.info(f"ğŸ“Š Loading images for {patient_count} patients...")
                
                for pid in tqdm(hf.keys(), desc="Loading cached images"):
                    patient_group = hf[pid]
                    image_data = {}
                    
                    # åŠ è½½å·¦å³ä¹³å›¾åƒ
                    if 'left_images' in patient_group:
                        image_data['left_images'] = patient_group['left_images'][:]
                    if 'right_images' in patient_group:
                        image_data['right_images'] = patient_group['right_images'][:]
                    
                    bilateral_image_features[pid] = image_data
            
            logger.info(f"âœ… Image data loaded: {len(bilateral_image_features)} patients")
            
            # åŠ è½½æ˜ å°„æ•°æ®
            with gzip.open(cache_files['mapping'], 'rb') as f:
                mapping_data = pickle.load(f)
            logger.info(f"âœ… Mapping data loaded")
            
            # é‡æ„æ•°æ®
            cached_data = {
                'clinical_features': clinical_df,
                'bilateral_image_features': bilateral_image_features,
                'bilateral_slices_data': mapping_data.get('bilateral_slices_data', {}),
                'processing_config': mapping_data.get('processing_config', {})
            }
            
            return cached_data
            
        except Exception as e:
            logger.error(f"âŒ Cache loading failed: {e}")
            raise

class IndependentAttentionLayer(Layer):
    """ç‹¬ç«‹çš„æ³¨æ„åŠ›æœºåˆ¶å±‚ï¼Œä¸“é—¨å¤„ç†å•ä¾§ä¹³è…º"""
    
    def __init__(self, dim, side_name='', **kwargs):
        super(IndependentAttentionLayer, self).__init__(**kwargs)
        self.dim = dim
        self.side_name = side_name
        
    def build(self, input_shape):
        # input_shapeæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«[features_shape, mask_shape]
        # æˆ‘ä»¬éœ€è¦ç¬¬ä¸€ä¸ªè¾“å…¥ï¼ˆfeaturesï¼‰çš„å½¢çŠ¶
        if isinstance(input_shape, list):
            features_shape = input_shape[0]
        else:
            features_shape = input_shape
            
        # è·å–ç‰¹å¾ç»´åº¦
        feature_dim = int(features_shape[-1])
        
        # ä¸ºæ¯ä¸€ä¾§åˆ›å»ºç‹¬ç«‹çš„æƒé‡
        self.W = self.add_weight(
            name=f'{self.side_name}_attention_weight',
            shape=(feature_dim, self.dim),
            initializer='glorot_uniform',
            trainable=True
        )
        self.b = self.add_weight(
            name=f'{self.side_name}_attention_bias',
            shape=(self.dim,),
            initializer='zeros',
            trainable=True
        )
        self.u = self.add_weight(
            name=f'{self.side_name}_attention_u',
            shape=(self.dim, 1),
            initializer='glorot_uniform',
            trainable=True
        )
        super(IndependentAttentionLayer, self).build(input_shape)
    
    def call(self, inputs):
        # è§£åŒ…è¾“å…¥
        if isinstance(inputs, list):
            x, mask = inputs
        else:
            x = inputs
            mask = K.ones_like(x[:, :, 0])  # å¦‚æœæ²¡æœ‰maskï¼Œåˆ›å»ºå…¨1çš„mask
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        uit = K.tanh(K.dot(x, self.W) + self.b)
        ait = K.dot(uit, self.u)
        ait = K.squeeze(ait, -1)
        
        # åº”ç”¨mask - å°†æ— æ•ˆä½ç½®è®¾ä¸ºæå°å€¼
        mask_value = -1e10
        ait = ait * mask + (1 - mask) * mask_value
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        ait = K.exp(ait)
        ait = ait / (K.sum(ait, axis=1, keepdims=True) + K.epsilon())
        ait = K.expand_dims(ait, axis=-1)
        
        # åŠ æƒå¹³å‡
        weighted_input = x * ait
        output = K.sum(weighted_input, axis=1)
        
        return output, ait
    
    def compute_output_shape(self, input_shape):
        if isinstance(input_shape, list):
            features_shape = input_shape[0]
        else:
            features_shape = input_shape
            
        return [(features_shape[0], features_shape[-1]), 
                (features_shape[0], features_shape[1], 1)]

class ImprovedBilateralAsymmetryLayer(Layer):
    """æ”¹è¿›çš„åŒä¾§ä¸å¯¹ç§°ç‰¹å¾å­¦ä¹ å±‚"""
    
    def __init__(self, units=64, **kwargs):
        super(ImprovedBilateralAsymmetryLayer, self).__init__(**kwargs)
        self.units = units
        
    def build(self, input_shape):
        # è¾“å…¥: [left_features, right_features]
        feature_dim = input_shape[0][-1]
        
        # ç‹¬ç«‹çš„ç‰¹å¾æå–ç½‘ç»œ
        self.left_transform = Dense(self.units, activation='relu', name='left_transform')
        self.right_transform = Dense(self.units, activation='relu', name='right_transform')
        
        # ä¸å¯¹ç§°ç‰¹å¾æå–ç½‘ç»œ
        self.asymmetry_dense1 = Dense(self.units, activation='relu', name='asymmetry_dense1')
        self.asymmetry_bn1 = BatchNormalization(name='asymmetry_bn1')
        self.asymmetry_dropout1 = Dropout(0.3)
        
        self.asymmetry_dense2 = Dense(self.units // 2, activation='relu', name='asymmetry_dense2')
        self.asymmetry_bn2 = BatchNormalization(name='asymmetry_bn2')
        
        # èåˆå±‚
        self.fusion_dense = Dense(self.units // 2, activation='relu', name='fusion_dense')
        
        super(ImprovedBilateralAsymmetryLayer, self).build(input_shape)
    
    def call(self, inputs, training=None):
        left_features, right_features = inputs
        
        # ç‹¬ç«‹å˜æ¢
        left_transformed = self.left_transform(left_features)
        right_transformed = self.right_transform(right_features)
        
        # è®¡ç®—å¤šç§ä¸å¯¹ç§°ç‰¹å¾
        # 1. ç›´æ¥å·®å¼‚
        diff_features = Subtract()([left_transformed, right_transformed])
        
        # 2. ç»å¯¹å·®å¼‚
        abs_diff_features = Lambda(lambda x: K.abs(x))(diff_features)
        
        # 3. å½’ä¸€åŒ–å·®å¼‚
        sum_features = Add()([left_transformed, right_transformed])
        normalized_diff = Lambda(lambda x: x[0] / (x[1] + K.epsilon()))([diff_features, sum_features])
        
        # 4. æœ€å¤§æœ€å°ç‰¹å¾
        max_features = Lambda(lambda x: K.maximum(x[0], x[1]))([left_transformed, right_transformed])
        min_features = Lambda(lambda x: K.minimum(x[0], x[1]))([left_transformed, right_transformed])
        
        # ç»„åˆæ‰€æœ‰ä¸å¯¹ç§°ç‰¹å¾
        asymmetry_features = Concatenate()([
            abs_diff_features,
            normalized_diff,
            max_features,
            min_features
        ])
        
        # é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ ä¸å¯¹ç§°æ¨¡å¼
        x = self.asymmetry_dense1(asymmetry_features)
        x = self.asymmetry_bn1(x, training=training)
        x = self.asymmetry_dropout1(x, training=training)
        
        x = self.asymmetry_dense2(x)
        x = self.asymmetry_bn2(x, training=training)
        
        # èåˆåŸå§‹ç‰¹å¾å’Œä¸å¯¹ç§°ç‰¹å¾
        combined = Concatenate()([left_transformed, right_transformed, x])
        output = self.fusion_dense(combined)
        
        return output
class BilateralMILBreastCancerModelV2:
    """æ”¹è¿›ç‰ˆåŒä¾§ä¹³è…ºMILæ¨¡å‹ - ä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, instance_shape=(128, 128, 3), max_instances=20, 
                 clinical_dim=6, num_classes=2):
        self.instance_shape = instance_shape
        self.max_instances = max_instances
        self.clinical_dim = clinical_dim
        self.num_classes = num_classes
        self.model = None
        self._build_model()
    
    def _build_instance_encoder(self):
        """ä½¿ç”¨MobileNetV2ä½œä¸ºç‰¹å¾æå–å™¨"""
        from tensorflow.keras.applications import MobileNetV2
        
        inputs = Input(shape=self.instance_shape)
        
        base_model = MobileNetV2(
            input_shape=self.instance_shape,
            include_top=False,
            weights='imagenet',
            pooling='avg',
            alpha=0.5
        )
        
        # å†»ç»“å¤§éƒ¨åˆ†å±‚
        for layer in base_model.layers[:-10]:
            layer.trainable = False
        
        x = base_model(inputs)
        x = Dense(128, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)
        
        return Model(inputs=inputs, outputs=x, name='instance_encoder')

    def _build_model(self):
        """æ„å»ºä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶çš„åŒä¾§MILæ¨¡å‹"""
        # è¾“å…¥å®šä¹‰
        bag_input = Input(shape=(self.max_instances, *self.instance_shape), name='bilateral_bag_input')
        instance_mask = Input(shape=(self.max_instances,), name='instance_mask')
        clinical_input = Input(shape=(self.clinical_dim,), name='clinical_input')
        side_mask = Input(shape=(self.max_instances,), name='side_mask')
        
        # æ„å»ºå®ä¾‹ç¼–ç å™¨
        instance_encoder = self._build_instance_encoder()
        
        # å¤„ç†æ¯ä¸ªå®ä¾‹
        instance_features_list = []
        for i in range(self.max_instances):
            instance = Lambda(lambda x: x[:, i, :, :, :])(bag_input)
            instance_feat = instance_encoder(instance)
            instance_features_list.append(instance_feat)
        
        # å †å æ‰€æœ‰å®ä¾‹ç‰¹å¾
        instance_features = Lambda(lambda x: K.stack(x, axis=1))(instance_features_list)
        
        # åº”ç”¨instance mask
        mask_expanded = Lambda(lambda x: K.expand_dims(x, axis=-1))(instance_mask)
        instance_features_masked = Multiply()([instance_features, mask_expanded])
        
        # åˆ›å»ºå·¦å³ä¾§çš„mask
        left_mask = Lambda(lambda x: x[0] * (1 - x[1]))([instance_mask, side_mask])
        right_mask = Lambda(lambda x: x[0] * x[1])([instance_mask, side_mask])
        
        # åˆ†ç¦»å·¦å³ä¾§ç‰¹å¾
        left_mask_expanded = Lambda(lambda x: K.expand_dims(x, axis=-1))(left_mask)
        right_mask_expanded = Lambda(lambda x: K.expand_dims(x, axis=-1))(right_mask)
        
        left_features = Multiply()([instance_features, left_mask_expanded])
        right_features = Multiply()([instance_features, right_mask_expanded])
        
        # ä½¿ç”¨ç‹¬ç«‹çš„æ³¨æ„åŠ›æœºåˆ¶
        left_attention_layer = IndependentAttentionLayer(64, side_name='left', name='left_attention')
        right_attention_layer = IndependentAttentionLayer(64, side_name='right', name='right_attention')
        
        # åˆ†åˆ«è®¡ç®—å·¦å³ä¾§çš„æ³¨æ„åŠ›
        left_bag_features, left_attention = left_attention_layer([left_features, left_mask])
        right_bag_features, right_attention = right_attention_layer([right_features, right_mask])
        
        # æ·»åŠ æ­£åˆ™åŒ–ï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€äº›ç‰¹å¾è¢«æå–
        left_bag_features = Lambda(lambda x: x + K.epsilon())(left_bag_features)
        right_bag_features = Lambda(lambda x: x + K.epsilon())(right_bag_features)
        
        # å­¦ä¹ åŒä¾§ä¸å¯¹ç§°ç‰¹å¾
        asymmetry_features = ImprovedBilateralAsymmetryLayer(units=32, name='bilateral_asymmetry')(
            [left_bag_features, right_bag_features]
        )
        
        # ç‹¬ç«‹å¤„ç†å·¦å³ç‰¹å¾
        left_processed = Dense(64, activation='relu', name='left_processing')(left_bag_features)
        left_processed = BatchNormalization()(left_processed)
        left_processed = Dropout(0.3)(left_processed)
        
        right_processed = Dense(64, activation='relu', name='right_processing')(right_bag_features)
        right_processed = BatchNormalization()(right_processed)
        right_processed = Dropout(0.3)(right_processed)
        
        # åˆå¹¶æ‰€æœ‰å›¾åƒç‰¹å¾
        combined_image_features = Concatenate(name='combined_features')([
            left_processed, 
            right_processed,
            asymmetry_features
        ])
        
        # å¤„ç†ä¸´åºŠç‰¹å¾
        x_clinical = Dense(32, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(clinical_input)
        x_clinical = BatchNormalization()(x_clinical)
        x_clinical = LeakyReLU(alpha=0.1)(x_clinical)
        x_clinical = Dropout(0.3)(x_clinical)
        
        x_clinical = Dense(16, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(x_clinical)
        x_clinical = BatchNormalization()(x_clinical)
        x_clinical = LeakyReLU(alpha=0.1)(x_clinical)
        
        # ç‰¹å¾èåˆ
        all_features = Concatenate()([combined_image_features, x_clinical])
        
        # æœ€ç»ˆåˆ†ç±»å±‚
        fusion = Dense(128, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(all_features)
        fusion = BatchNormalization()(fusion)
        fusion = LeakyReLU(alpha=0.1)(fusion)
        fusion = Dropout(0.5)(fusion)
        
        fusion = Dense(64, kernel_regularizer=l1_l2(l1=0.02, l2=0.02))(fusion)
        fusion = BatchNormalization()(fusion)
        fusion = LeakyReLU(alpha=0.1)(fusion)
        fusion = Dropout(0.4)(fusion)
        
        fusion = Dense(32, kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(fusion)
        fusion = BatchNormalization()(fusion)
        fusion = LeakyReLU(alpha=0.1)(fusion)
        
        # è¾“å‡ºå±‚
        output = Dense(self.num_classes, activation='softmax', name='bilateral_risk_output')(fusion)
        
        # æ„å»ºæ¨¡å‹
        self.model = Model(
            inputs=[bag_input, instance_mask, clinical_input, side_mask], 
            outputs=output,
            name='Bilateral_MIL_V2_Independent_Attention'
        )
        
        # æ³¨æ„åŠ›æ¨¡å‹ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        self.attention_model = Model(
            inputs=[bag_input, instance_mask, clinical_input, side_mask],
            outputs=[output, left_attention, right_attention, left_bag_features, right_bag_features, asymmetry_features],
            name='Bilateral_MIL_V2_Attention_Model'
        )
        
        # ç¼–è¯‘æ¨¡å‹
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        logger.info(f"âœ… Bilateral MIL V2 model built with independent attention")
        logger.info(f"   Total parameters: {self.model.count_params():,}")
        logger.info(f"   Using separate attention mechanisms for left and right breast")

# æ·»åŠ ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥éªŒè¯æ³¨æ„åŠ›åˆ†å¸ƒ
def validate_attention_distribution(model, test_data, num_samples=5):
    """éªŒè¯æ”¹è¿›åæ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒ"""
    logger.info("\nğŸ” Validating attention distribution for improved model...")
    
    X_test = [
        test_data['bags'][:num_samples],
        test_data['instance_masks'][:num_samples],
        test_data['clinical_features'][:num_samples],
        test_data['side_masks'][:num_samples]
    ]
    
    # è·å–æ³¨æ„åŠ›è¾“å‡º
    outputs = model.attention_model.predict(X_test, verbose=0)
    predictions, left_attentions, right_attentions = outputs[:3]
    
    for i in range(num_samples):
        instance_mask = test_data['instance_masks'][i]
        side_mask = test_data['side_masks'][i]
        
        # è®¡ç®—å·¦å³ä¾§mask
        left_mask = instance_mask * (1 - side_mask)
        right_mask = instance_mask * side_mask
        
        # è·å–æœ‰æ•ˆçš„æ³¨æ„åŠ›æƒé‡
        left_valid_idx = np.where(left_mask > 0)[0]
        right_valid_idx = np.where(right_mask > 0)[0]
        
        if len(left_valid_idx) > 0:
            left_att_valid = left_attentions[i][left_valid_idx, 0]
            left_sum = np.sum(left_att_valid)
        else:
            left_sum = 0
            
        if len(right_valid_idx) > 0:
            right_att_valid = right_attentions[i][right_valid_idx, 0]
            right_sum = np.sum(right_att_valid)
        else:
            right_sum = 0
        
        logger.info(f"\nSample {i}:")
        logger.info(f"  Left slices: {len(left_valid_idx)}, attention sum: {left_sum:.3f}")
        logger.info(f"  Right slices: {len(right_valid_idx)}, attention sum: {right_sum:.3f}")
        logger.info(f"  Prediction: {predictions[i]}")

class BilateralMILDataManager:
    """åŒä¾§ä¹³è…ºMILæ•°æ®ç®¡ç†å™¨ - ä¸ä½¿ç”¨BI-RADSè¿›è¡Œè®­ç»ƒ"""
    
    def __init__(self, config):
        self.config = config
        self.scaler = StandardScaler()
        self.cache_manager = FixedCacheManager(config.get('cache_root', './cache'))
        
        # é£é™©æ˜ å°„ - ä»…ç”¨äºåˆ›å»ºæ ‡ç­¾ï¼Œä¸ç”¨äºç‰¹å¾
        # BI-RADS 3-4 â†’ 0 (ä¸­é£é™©)
        # BI-RADS 5-6 â†’ 1 (é«˜é£é™©)
        self.risk_mapping = {3: 0, 4: 0, 5: 1, 6: 1}
        self.risk_names = {0: 'Medium Risk', 1: 'High Risk'}
        self.num_classes = 2  # äºŒåˆ†ç±»
        
        # è¦å¿½ç•¥çš„BI-RADSç­‰çº§
        self.ignore_birads = [1, 2]
        
        # MILé…ç½®
        self.max_instances = config.get('max_instances', 20)
        
        logger.info(f"âœ… Bilateral MIL data manager initialized")
        logger.info(f"   Max instances per bilateral bag: {self.max_instances}")
        logger.info(f"   NOT using BI-RADS asymmetry as feature - model will learn from images")
    
    def prepare_bilateral_mil_data(self, clinical_df, bilateral_image_features):
        """å‡†å¤‡åŒä¾§ä¹³è…ºMILæ ¼å¼çš„æ•°æ® - ä¸ä½¿ç”¨BI-RADSä½œä¸ºç‰¹å¾"""
        bags = []  # æ¯ä¸ªåŒ…æ˜¯ä¸€ä¸ªæ‚£è€…çš„å·¦å³ä¹³è…ºæ‰€æœ‰åˆ‡ç‰‡
        instance_masks = []  # æ ‡è®°æœ‰æ•ˆåˆ‡ç‰‡
        side_masks = []  # æ ‡è®°å·¦å³ä¾§
        clinical_features = []
        risk_labels = []
        bag_info = []
        
        logger.info(f"ğŸ“Š Preparing Bilateral MIL data: {len(bilateral_image_features)} patients")
        
        total_patients = 0
        slice_counts = []
        
        for pid, image_data in tqdm(bilateral_image_features.items(), desc="Preparing bilateral MIL bags"):
            patient_row = clinical_df[clinical_df['PID'] == pid]
            
            if len(patient_row) == 0:
                continue
            
            patient_clinical = patient_row.iloc[0]
            
            # æ£€æŸ¥BI-RADSæ ‡ç­¾ - ä»…ç”¨äºåˆ›å»ºground truthï¼Œä¸ä½œä¸ºç‰¹å¾
            birads_left = patient_clinical.get('BI-RADSl')
            birads_right = patient_clinical.get('BI-RADSr')
            
            if pd.isna(birads_left) or pd.isna(birads_right):
                continue
                
            # è½¬æ¢ä¸ºæ•´æ•°
            birads_left = int(birads_left)
            birads_right = int(birads_right)
            
            # å¿½ç•¥BI-RADS 1å’Œ2çš„æ•°æ®
            if birads_left in self.ignore_birads and birads_right in self.ignore_birads:
                continue
            
            # è·å–å·¦å³ä¹³è…ºå›¾åƒ
            left_images = image_data.get('left_images', [])
            right_images = image_data.get('right_images', [])
            
            # å¿…é¡»è‡³å°‘æœ‰ä¸€ä¾§æœ‰å›¾åƒæ•°æ®
            if len(left_images) == 0 and len(right_images) == 0:
                continue
            
            # æå–ä¸´åºŠç‰¹å¾ï¼ˆç§»é™¤BI-RADSä¸å¯¹ç§°ç‰¹å¾ï¼‰
            try:
                age = float(patient_clinical['å¹´é¾„'])
                bmi = float(patient_clinical['BMI'])
                density = float(patient_clinical['density_numeric'])
                history = float(patient_clinical['history'])
                
                # æ·»åŠ è¡ç”Ÿç‰¹å¾
                age_group = age // 10
                bmi_category = 0 if bmi < 18.5 else (1 if bmi < 25 else (2 if bmi < 30 else 3))
                
                clinical_feature = np.array([
                    age, bmi, density, history,
                    age_group, bmi_category
                    # ç§»é™¤äº†: age * density å’Œ birads_asymmetry
                ], dtype=np.float32)
            except (ValueError, TypeError):
                continue
            
            # åˆå¹¶å·¦å³ä¹³è…ºå›¾åƒï¼Œåˆ›å»ºåŒä¾§åŒ…
            bilateral_slices = []
            side_indicators = []  # 0è¡¨ç¤ºå·¦ä¾§ï¼Œ1è¡¨ç¤ºå³ä¾§
            slice_positions = []  # è®°å½•æ¯ä¸ªåˆ‡ç‰‡çš„ä½ç½®ä¿¡æ¯
            
            # æ·»åŠ å·¦ä¹³åˆ‡ç‰‡
            for i, left_img in enumerate(left_images):
                bilateral_slices.append(left_img)
                side_indicators.append(0)  # å·¦ä¾§æ ‡è®°ä¸º0
                slice_positions.append(('left', i))
            
            # æ·»åŠ å³ä¹³åˆ‡ç‰‡
            for i, right_img in enumerate(right_images):
                bilateral_slices.append(right_img)
                side_indicators.append(1)  # å³ä¾§æ ‡è®°ä¸º1
                slice_positions.append(('right', i))
            
            # è®¡ç®—æ•´ä½“é£é™©ï¼ˆå–ä¸¤ä¾§æœ€é«˜é£é™©ï¼‰- ä»…ç”¨ä½œæ ‡ç­¾
            risk_left = self.risk_mapping.get(birads_left, 0) if birads_left not in self.ignore_birads else 0
            risk_right = self.risk_mapping.get(birads_right, 0) if birads_right not in self.ignore_birads else 0
            overall_risk = max(risk_left, risk_right)
            
            # æ ‡å‡†åŒ–åŒ…çš„å¤§å°
            bag, mask, side_mask = self._standardize_bilateral_bag(bilateral_slices, side_indicators)
            
            bags.append(bag)
            instance_masks.append(mask)
            side_masks.append(side_mask)
            clinical_features.append(clinical_feature)
            risk_labels.append(overall_risk)
            
            # å­˜å‚¨ä¿¡æ¯ç”¨äºåˆ†æï¼ˆä½†ä¸ä½œä¸ºè®­ç»ƒç‰¹å¾ï¼‰
            bag_info.append({
                'patient_id': pid,
                'n_left_instances': len(left_images),
                'n_right_instances': len(right_images),
                'n_total_instances': len(bilateral_slices),
                'birads_left': birads_left,
                'birads_right': birads_right,
                'overall_risk': overall_risk,
                'slice_positions': slice_positions[:len(bilateral_slices)]
            })
            
            total_patients += 1
            slice_counts.append(len(bilateral_slices))
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ“Š Bilateral MIL data prepared:")
        logger.info(f"   Total patients: {total_patients}")
        logger.info(f"   Ignored BI-RADS 1-2 cases")
        logger.info(f"   Only including BI-RADS 3-6")
        logger.info(f"   Clinical features: {clinical_features[0].shape[0]} dimensions (no BI-RADS asymmetry)")
        if slice_counts:
            logger.info(f"   Average slices per patient: {np.mean(slice_counts):.1f}")
            logger.info(f"   Min slices: {np.min(slice_counts)}")
            logger.info(f"   Max slices: {np.max(slice_counts)}")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        mil_data = {
            'bags': np.array(bags),
            'instance_masks': np.array(instance_masks),
            'side_masks': np.array(side_masks),
            'clinical_features': np.array(clinical_features),
            'risk_labels': np.array(risk_labels),
            'bag_info': bag_info
        }
        
        return mil_data
    
    def _standardize_bilateral_bag(self, slices, side_indicators):
        """æ ‡å‡†åŒ–åŒä¾§åŒ…çš„å¤§å°ï¼ŒåŒ…æ‹¬ä¾§åˆ«æ ‡è®°"""
        n_slices = len(slices)
        
        if n_slices >= self.max_instances:
            # å‡åŒ€é‡‡æ ·
            indices = np.linspace(0, n_slices-1, self.max_instances, dtype=int)
            selected_slices = [slices[i] for i in indices]
            selected_sides = [side_indicators[i] for i in indices]
            mask = np.ones(self.max_instances)
        else:
            # å¡«å……
            selected_slices = list(slices)
            selected_sides = list(side_indicators)
            padding_needed = self.max_instances - n_slices
            
            # ç”¨é›¶å¡«å……
            padding_shape = slices[0].shape
            for _ in range(padding_needed):
                selected_slices.append(np.zeros(padding_shape, dtype=np.float32))
                selected_sides.append(0)  # å¡«å……çš„æ ‡è®°ä¸º0
            
            # åˆ›å»ºmask
            mask = np.zeros(self.max_instances)
            mask[:n_slices] = 1
        
        return np.array(selected_slices), mask, np.array(selected_sides)
    
    def load_and_prepare_data(self, force_rebuild=False):
        """åŠ è½½æ•°æ®å¹¶å‡†å¤‡åŒä¾§MILæ ¼å¼"""
        
        print("=" * 80)
        print("ğŸ—„ï¸ BILATERAL MIL MODEL DATA LOADING")
        print("ğŸ–¼ï¸ Model will learn asymmetry features from images directly")
        print("=" * 80)
        
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_manager.cache_exists() and not force_rebuild:
            logger.info("ğŸ¯ CACHE FOUND! Loading...")
            try:
                cached_data = self.cache_manager.load_cache()
                mil_data = self.prepare_bilateral_mil_data(
                    cached_data['clinical_features'],
                    cached_data['bilateral_image_features']
                )
                
                print("=" * 80)
                print("ğŸ‰ SUCCESS! Data loaded from cache and prepared for Bilateral MIL")
                print(f"âœ… Loaded {len(mil_data['bags'])} patients")
                print("âœ… Model will learn bilateral asymmetry from images automatically")
                print("=" * 80)
                
                self._print_data_summary(mil_data)
                return mil_data
                
            except Exception as e:
                logger.warning(f"âš ï¸ Cache loading failed: {e}")
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œéœ€è¦å¤„ç†åŸå§‹æ•°æ®
        logger.error("âŒ No cache found. Please run the original model first to create cache.")
        return None
    
    def _print_data_summary(self, mil_data):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        logger.info("ğŸ“Š Bilateral MIL Data Summary:")
        logger.info(f"   Total patients: {len(mil_data['bags'])}")
        logger.info(f"   Bilateral bag shape: {mil_data['bags'].shape}")
        logger.info(f"   Clinical features: {mil_data['clinical_features'].shape[1]} dimensions")
        logger.info(f"   Side masks shape: {mil_data['side_masks'].shape}")
        
        # é£é™©åˆ†å¸ƒ
        labels = mil_data['risk_labels']
        unique_labels, counts = np.unique(labels, return_counts=True)
        
        logger.info(f"   Risk distribution:")
        for label, count in zip(unique_labels, counts):
            risk_name = self.risk_names[label]
            percentage = count / len(labels) * 100
            logger.info(f"     {risk_name}: {count} ({percentage:.1f}%)")
        
        # åˆ‡ç‰‡åˆ†å¸ƒåˆ†æ
        left_counts = [info['n_left_instances'] for info in mil_data['bag_info']]
        right_counts = [info['n_right_instances'] for info in mil_data['bag_info']]
        total_counts = [info['n_total_instances'] for info in mil_data['bag_info']]
        
        logger.info(f"   Slice distribution:")
        logger.info(f"     Left slices per patient: {np.mean(left_counts):.1f} Â± {np.std(left_counts):.1f}")
        logger.info(f"     Right slices per patient: {np.mean(right_counts):.1f} Â± {np.std(right_counts):.1f}")
        logger.info(f"     Total slices per patient: {np.mean(total_counts):.1f} Â± {np.std(total_counts):.1f}")
        logger.info(f"   Note: BI-RADS asymmetry is NOT used as feature - model learns from images")

class BilateralImprovedEnsembleMILPipeline:
    """åŒä¾§ä¹³è…ºæ”¹è¿›çš„é›†æˆMILè®­ç»ƒæµç¨‹ - æ·±åº¦å­¦ä¹ ä¸å¯¹ç§°ç‰¹å¾"""
    
    def __init__(self, config):
        self.config = config
        self.output_dir = config['output_dir']
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_manager = BilateralMILDataManager(config)
        self.overfitting_analysis = {}
        self.ensemble_models = []
        
        logger.info("ğŸš€ Bilateral Improved MIL training pipeline initialized")
        logger.info("   Using deep learning to discover asymmetry patterns")

    def create_model_with_independent_attention(self, model_config):
        """åˆ›å»ºä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›æœºåˆ¶çš„åŒä¾§æ¨¡å‹"""
        model = BilateralMILBreastCancerModelV2(
            instance_shape=(*self.config['image_config']['target_size'], 3),
            max_instances=self.config.get('max_instances', 20),
            clinical_dim=6,  # ä¸åŒ…å«BI-RADSä¸å¯¹ç§°ç‰¹å¾
            num_classes=2
        )
        
        # é‡æ–°ç¼–è¯‘æ¨¡å‹
        model.model.compile(
            optimizer=Adam(learning_rate=model_config['lr']),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def _split_data_patient_aware(self, mil_data):
        """æ‚£è€…æ„ŸçŸ¥çš„æ•°æ®åˆ†å‰²"""
        # è·å–æ¯ä¸ªæ‚£è€…çš„é£é™©ç­‰çº§
        patient_ids = [info['patient_id'] for info in mil_data['bag_info']]
        patient_risks = [info['overall_risk'] for info in mil_data['bag_info']]
        
        # åˆ†å±‚åˆ†å‰²æ‚£è€…
        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
        train_idx, temp_idx = next(sss.split(range(len(patient_ids)), patient_risks))
        
        temp_risks = [patient_risks[i] for i in temp_idx]
        
        # å†åˆ†å‰²éªŒè¯é›†å’Œæµ‹è¯•é›†
        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
        val_idx_temp, test_idx_temp = next(sss_val.split(range(len(temp_idx)), temp_risks))
        
        val_idx = [temp_idx[i] for i in val_idx_temp]
        test_idx = [temp_idx[i] for i in test_idx_temp]
        
        def create_subset(indices):
            return {
                'bags': mil_data['bags'][indices],
                'instance_masks': mil_data['instance_masks'][indices],
                'side_masks': mil_data['side_masks'][indices],
                'clinical_features': mil_data['clinical_features'][indices],
                'risk_labels': mil_data['risk_labels'][indices],
                'bag_info': [mil_data['bag_info'][i] for i in indices]
            }
        
        train_data = create_subset(train_idx)
        val_data = create_subset(val_idx)
        test_data = create_subset(test_idx)
        
        logger.info(f"ğŸ“Š Patient-aware data split:")
        logger.info(f"   Train: {len(train_idx)} patients")
        logger.info(f"   Val: {len(val_idx)} patients")
        logger.info(f"   Test: {len(test_idx)} patients")
        
        return train_data, val_data, test_data
    
    def augment_minority_class(self, train_data):
        """æ•°æ®å¢å¼º - è®©é«˜é£é™©ç±»å 50%"""
        logger.info("ğŸ”„ Bilateral minority class augmentation...")
        
        high_risk_indices = np.where(train_data['risk_labels'] == 1)[0]
        medium_risk_indices = np.where(train_data['risk_labels'] == 0)[0]
        
        n_high = len(high_risk_indices)
        n_medium = len(medium_risk_indices)
        
        if n_high == 0:
            return train_data
        
        # ç›®æ ‡ï¼šè®©é«˜é£é™©æ ·æœ¬æ•°é‡ç­‰äºä¸­é£é™©æ ·æœ¬æ•°é‡
        target_high_samples = n_medium
        augment_factor = max(2, n_medium // n_high)
        
        logger.info(f"   Original - High: {n_high}, Medium: {n_medium}")
        logger.info(f"   Target - High: {target_high_samples}, Medium: {n_medium}")
        logger.info(f"   Augmentation factor: {augment_factor}x")
        
        augmented_bags = []
        augmented_masks = []
        augmented_side_masks = []
        augmented_clinical = []
        augmented_labels = []
        
        # å¯¹æ¯ä¸ªé«˜é£é™©æ ·æœ¬åˆ›å»ºå¤šä¸ªå¼ºå¢å¼ºç‰ˆæœ¬
        for idx in high_risk_indices:
            original_bag = train_data['bags'][idx]
            original_mask = train_data['instance_masks'][idx]
            original_side_mask = train_data['side_masks'][idx]
            
            for i in range(augment_factor):
                bag = original_bag.copy()
                
                # å¼ºå¢å¼º
                for j in range(len(bag)):
                    if original_mask[j] > 0:
                        # ç»„åˆå¤šç§å¢å¼º
                        # 1. å¼ºå™ªå£°
                        noise = np.random.normal(0, 0.05, bag[j].shape)
                        bag[j] = np.clip(bag[j] + noise, 0, 1)
                        
                        # 2. éšæœºæ“¦é™¤
                        if np.random.random() > 0.5:
                            h, w = bag[j].shape[:2]
                            erase_h = np.random.randint(h//8, h//4)
                            erase_w = np.random.randint(w//8, w//4)
                            y = np.random.randint(0, h - erase_h)
                            x = np.random.randint(0, w - erase_w)
                            bag[j][y:y+erase_h, x:x+erase_w] = np.random.random()
                        
                        # 3. è‰²å½©æŠ–åŠ¨
                        color_shift = np.random.uniform(-0.1, 0.1, bag[j].shape)
                        bag[j] = np.clip(bag[j] + color_shift, 0, 1)
                
                augmented_bags.append(bag)
                augmented_masks.append(original_mask)
                augmented_side_masks.append(original_side_mask)
                
                # ä¸´åºŠç‰¹å¾ä¹ŸåŠ æ‰°åŠ¨
                clinical_noise = np.random.normal(0, 0.1, train_data['clinical_features'][idx].shape)
                augmented_clinical.append(train_data['clinical_features'][idx] + clinical_noise)
                augmented_labels.append(1)
        
        # æ·»åŠ æ‰€æœ‰ä¸­é£é™©æ ·æœ¬
        for idx in medium_risk_indices:
            augmented_bags.append(train_data['bags'][idx])
            augmented_masks.append(train_data['instance_masks'][idx])
            augmented_side_masks.append(train_data['side_masks'][idx])
            augmented_clinical.append(train_data['clinical_features'][idx])
            augmented_labels.append(0)
        
        # åˆ›å»ºå¹³è¡¡çš„æ•°æ®é›†
        augmented_data = {
            'bags': np.array(augmented_bags),
            'instance_masks': np.array(augmented_masks),
            'side_masks': np.array(augmented_side_masks),
            'clinical_features': np.array(augmented_clinical),
            'risk_labels': np.array(augmented_labels),
            'bag_info': []
        }
        
        # æ‰“ä¹±
        indices = np.random.permutation(len(augmented_data['bags']))
        for key in ['bags', 'instance_masks', 'side_masks', 'clinical_features', 'risk_labels']:
            augmented_data[key] = augmented_data[key][indices]
        
        # ç»Ÿè®¡
        unique_labels, counts = np.unique(augmented_data['risk_labels'], return_counts=True)
        logger.info(f"âœ… Final distribution:")
        for label, count in zip(unique_labels, counts):
            logger.info(f"   Class {label}: {count} ({count/len(augmented_data['bags'])*100:.1f}%)")
        
        return augmented_data
    
    def train_single_model(self, model, train_data, val_data, class_weight, model_name):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        logger.info(f"ğŸƒ Training bilateral {model_name} model...")
        
        X_train = [
            train_data['bags'],
            train_data['instance_masks'],
            train_data['clinical_features'],
            train_data['side_masks']  # æ–°å¢side_masks
        ]
        y_train = train_data['risk_labels']
        
        X_val = [
            val_data['bags'],
            val_data['instance_masks'],
            val_data['clinical_features'],
            val_data['side_masks']  # æ–°å¢side_masks
        ]
        y_val = val_data['risk_labels']
        
        # ç®€åŒ–çš„å›è°ƒ
        callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=50,  # å¢åŠ åˆ°50
        restore_best_weights=True,
        verbose=1,
        min_delta=0.001  # æ·»åŠ æœ€å°æ”¹å–„é˜ˆå€¼
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=20,  # å¢åŠ åˆ°20
        min_lr=1e-8,
        verbose=1
    )
]
        
        history = model.model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=4,  # å‡å°batch sizeå› ä¸ºåŒ…æ›´å¤§äº†
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            class_weight=class_weight,
            verbose=0
        )
        
        return history.history
    
    def detect_overfitting(self, history, model_name):
        """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
        train_acc = history['accuracy']
        val_acc = history['val_accuracy']
        train_loss = history['loss']
        val_loss = history['val_loss']
        
        # è®¡ç®—è¿‡æ‹ŸåˆæŒ‡æ ‡
        final_gap = train_acc[-1] - val_acc[-1]
        max_gap = max([train_acc[i] - val_acc[i] for i in range(len(train_acc))])
        
        # æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦ä¸Šå‡
        val_loss_increasing = False
        if len(val_loss) > 10:
            recent_trend = np.polyfit(range(5), val_loss[-5:], 1)[0]
            val_loss_increasing = recent_trend > 0
        
        # åªæœ‰å½“è®­ç»ƒå‡†ç¡®ç‡æ˜æ˜¾é«˜äºéªŒè¯å‡†ç¡®ç‡æ—¶æ‰æ˜¯è¿‡æ‹Ÿåˆ
        overfitting_detected = (
            final_gap > 0.15 and  # æ”¹ä¸º andï¼Œå¹¶ä¸”åªè€ƒè™‘æ­£çš„gap
            max_gap > 0.20 and    
            val_loss_increasing  
        )
        
        analysis = {
            'model_name': model_name,
            'final_train_acc': train_acc[-1],
            'final_val_acc': val_acc[-1],
            'accuracy_gap': final_gap,
            'max_accuracy_gap': max_gap,
            'val_loss_increasing': val_loss_increasing,
            'overfitting_detected': overfitting_detected,
            'best_epoch': np.argmax(val_acc)
        }
        
        if overfitting_detected:
            logger.warning(f"âš ï¸ Overfitting detected in {model_name}!")
            logger.warning(f"   Train/Val gap: {final_gap:.3f}")
            logger.warning(f"   Best epoch was: {analysis['best_epoch']+1}")
        
        return analysis
    
    def find_optimal_threshold_for_model(self, model, test_data):
        """ä¸ºå•ä¸ªæ¨¡å‹æ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼"""
        X_test = [
            test_data['bags'],
            test_data['instance_masks'],
            test_data['clinical_features'],
            test_data['side_masks']
        ]
        y_test = test_data['risk_labels']
        
        predictions = model.model.predict(X_test, verbose=0)
        
        best_score = 0
        best_threshold = 0.5
        
        for threshold in np.arange(0.1, 0.9, 0.05):
            pred_classes = (predictions[:, 1] > threshold).astype(int)
            cm = confusion_matrix(y_test, pred_classes)
            
            if cm.shape[0] > 1 and cm.shape[1] > 1:
                med_recall = cm[0, 0] / cm[0, :].sum() if cm[0, :].sum() > 0 else 0
                high_recall = cm[1, 1] / cm[1, :].sum() if cm[1, :].sum() > 0 else 0
                
                # å¹³è¡¡åˆ†æ•°ï¼ˆå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æƒé‡ï¼‰
                balanced_score = 0.4 * med_recall + 0.6 * high_recall
                
                if balanced_score > best_score:
                    best_score = balanced_score
                    best_threshold = threshold
        
        return best_threshold
    
    def evaluate_on_test_set(self, model, test_data, model_name, threshold=None):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å•ä¸ªæ¨¡å‹"""
        X_test = [
            test_data['bags'],
            test_data['instance_masks'],
            test_data['clinical_features'],
            test_data['side_masks']
        ]
        y_test = test_data['risk_labels']
        
        predictions = model.model.predict(X_test, verbose=0)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šé˜ˆå€¼ï¼Œæ‰¾åˆ°æœ€ä¼˜é˜ˆå€¼
        if threshold is None:
            threshold = self.find_optimal_threshold_for_model(model, test_data)
        
        pred_classes = (predictions[:, 1] > threshold).astype(int)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        accuracy = accuracy_score(y_test, pred_classes)
        cm = confusion_matrix(y_test, pred_classes)
        report = classification_report(y_test, pred_classes, 
                                    target_names=['Medium Risk', 'High Risk'],
                                    output_dict=True,
                                    zero_division=0)
        
        # è®¡ç®—é¢å¤–çš„æŒ‡æ ‡
        if cm.shape[0] > 1 and cm.shape[1] > 1:
            med_recall = cm[0, 0] / cm[0, :].sum() if cm[0, :].sum() > 0 else 0
            high_recall = cm[1, 1] / cm[1, :].sum() if cm[1, :].sum() > 0 else 0
            med_precision = cm[0, 0] / cm[:, 0].sum() if cm[:, 0].sum() > 0 else 0
            high_precision = cm[1, 1] / cm[:, 1].sum() if cm[:, 1].sum() > 0 else 0
            
            # è®¡ç®—å¹³è¡¡æŒ‡æ ‡
            balanced_accuracy = (med_recall + high_recall) / 2
            # åŠ æƒF1åˆ†æ•°ï¼ˆæ›´é‡è§†é«˜é£é™©ï¼‰
            weighted_f1 = 0.3 * report['Medium Risk']['f1-score'] + 0.7 * report['High Risk']['f1-score']
        else:
            balanced_accuracy = accuracy
            weighted_f1 = 0
            med_recall = high_recall = 0
            med_precision = high_precision = 0
        
        results = {
            'model_name': model_name,
            'accuracy': accuracy,
            'balanced_accuracy': balanced_accuracy,
            'weighted_f1': weighted_f1,
            'threshold': threshold,
            'confusion_matrix': cm,
            'classification_report': report,
            'medium_recall': med_recall,
            'high_recall': high_recall,
            'medium_precision': med_precision,
            'high_precision': high_precision
        }
        
        logger.info(f"\nğŸ“Š Test Set Results for Bilateral {model_name}:")
        logger.info(f"   Accuracy: {accuracy:.3f}")
        logger.info(f"   Balanced Accuracy: {balanced_accuracy:.3f}")
        logger.info(f"   Medium Risk Recall: {med_recall:.3f}")
        logger.info(f"   High Risk Recall: {high_recall:.3f}")
        logger.info(f"   Optimal Threshold: {threshold:.2f}")
        
        # æ‰“å°æ ¼å¼åŒ–çš„æ··æ·†çŸ©é˜µ
        logger.info(f"   Confusion Matrix:")
        confusion_matrix_str = format_confusion_matrix(cm)
        for line in confusion_matrix_str.split('\n'):
            if line.strip():
                logger.info(f"   {line}")
        
        return results
    
    def select_best_model(self, test_results):
        """åŸºäºæµ‹è¯•é›†è¡¨ç°é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        logger.info("\nğŸ† Selecting best bilateral model based on TEST SET performance...")
        
        # å®šä¹‰é€‰æ‹©æ ‡å‡†
        selection_criteria = []
        
        for result in test_results:
            # è®¡ç®—ç»¼åˆåˆ†æ•°
            score = (
                0.3 * result['accuracy'] +           # æ•´ä½“å‡†ç¡®ç‡
                0.3 * result['balanced_accuracy'] +   # å¹³è¡¡å‡†ç¡®ç‡
                0.2 * result['weighted_f1'] +         # åŠ æƒF1
                0.2 * result['high_recall']           # é«˜é£é™©å¬å›ç‡
            )
            
            # æƒ©ç½šè¿‡æ‹Ÿåˆ
            model_name = result['model_name']
            if model_name in self.overfitting_analysis:
                if self.overfitting_analysis[model_name]['overfitting_detected']:
                    score *= 0.9  # é™ä½10%åˆ†æ•°
            
            selection_criteria.append({
                'model_name': model_name,
                'score': score,
                'metrics': result
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        selection_criteria.sort(key=lambda x: x['score'], reverse=True)
        
        best_model = selection_criteria[0]
        
        logger.info(f"\nâœ… Best bilateral model: {best_model['model_name']}")
        logger.info(f"   Score: {best_model['score']:.3f}")
        logger.info(f"   Test Accuracy: {best_model['metrics']['accuracy']:.3f}")
        logger.info(f"   High Risk Recall: {best_model['metrics']['high_recall']:.3f}")
        logger.info(f"   Medium Risk Recall: {best_model['metrics']['medium_recall']:.3f}")
        logger.info(f"   Note: Model learned asymmetry features from images automatically")
        
        return best_model
    
    def run_bilateral_ensemble_training_v2(self):
        """è¿è¡ŒåŒä¾§ä¹³è…ºé›†æˆè®­ç»ƒæµç¨‹ - ä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›ç‰ˆæœ¬"""
        logger.info("ğŸš€ Starting Bilateral Ensemble MIL training with INDEPENDENT attention...")
        logger.info("   Each breast will have its own attention mechanism")
        
        # 1. åŠ è½½æ•°æ®
        mil_data = self.data_manager.load_and_prepare_data()
        
        if mil_data is None or len(mil_data['bags']) < 10:
            logger.error("âŒ Insufficient data")
            return None
        
        # 2. æ•°æ®é¢„å¤„ç†
        self.data_manager.scaler.fit(mil_data['clinical_features'])
        mil_data['clinical_features'] = self.data_manager.scaler.transform(
            mil_data['clinical_features']
        )
        
        # 3. æ•°æ®åˆ†å‰²
        train_data, val_data, test_data = self._split_data_patient_aware(mil_data)
        
        logger.info(f"\nğŸ“Š Bilateral Data Split Summary:")
        logger.info(f"   Train: {len(train_data['bags'])} patients")
        logger.info(f"   Val: {len(val_data['bags'])} patients")
        logger.info(f"   Test: {len(test_data['bags'])} patients")
        
        # 4. è®­ç»ƒå¤šä¸ªåŒä¾§æ¨¡å‹ - ä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›
        all_histories = {}
        all_models = []
        test_results = []
        
        model_configs = [
    {
        'name': 'bilateral_independent_balanced_v2',
        'lr': 0.0001,  # é™ä½å­¦ä¹ ç‡
        'class_weight': {0: 1.0, 1: 1.0},  # å¹³è¡¡æƒé‡
    },
    {
        'name': 'bilateral_independent_high_risk_v2',
        'lr': 0.0001,
        'class_weight': {0: 1.0, 1: 2.5},  # æ¸©å’Œçš„é«˜é£é™©åå‘
    },
    {
        'name': 'bilateral_independent_conservative_v2',
        'lr': 0.00005,  # æ›´ä½çš„å­¦ä¹ ç‡
        'class_weight': {0: 2.0, 1: 1.0},  # æ¸©å’Œçš„ä¿å®ˆåå‘
    }
]
        
        for config in model_configs:
            logger.info(f"\n{'='*60}")
            logger.info(f"Training bilateral {config['name']} model with INDEPENDENT attention...")
            
            # åˆ›å»ºä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›çš„æ¨¡å‹
            model = self.create_model_with_independent_attention(config)
            
            # å¯é€‰ï¼šå¯¹é«˜é£é™©focusæ¨¡å‹ä½¿ç”¨æ•°æ®å¢å¼º
            if 'high_risk' in config['name']:
                augmented_train = self.augment_minority_class(train_data)
            else:
                augmented_train = train_data
            
            history = self.train_single_model(
                model, augmented_train, val_data,
                class_weight=config['class_weight'],
                model_name=config['name']
            )
            
            # æ£€æµ‹è¿‡æ‹Ÿåˆ
            overfitting_info = self.detect_overfitting(history, config['name'])
            self.overfitting_analysis[config['name']] = overfitting_info
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_result = self.evaluate_on_test_set(model, test_data, config['name'])
            
            # éªŒè¯æ³¨æ„åŠ›åˆ†å¸ƒ
            self._validate_model_attention(model, test_data, config['name'])
            
            all_histories[config['name']] = history
            all_models.append({
                'model': model,
                'config': config,
                'history': history,
                'test_performance': test_result
            })
            test_results.append(test_result)
            
            # ä¿å­˜æ¨¡å‹åˆ°å®ä¾‹å˜é‡
            self.ensemble_models = all_models
        
        # 5. é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_info = self.select_best_model(test_results)
        
        # 6. ä¿å­˜ç»“æœ
        results = {
            'best_model': best_model_info,
            'all_test_results': test_results,
            'overfitting_analysis': self.overfitting_analysis,
            'data_stats': {
                'train_size': len(train_data['bags']),
                'val_size': len(val_data['bags']),
                'test_size': len(test_data['bags'])
            },
            'test_data': test_data  # æ·»åŠ è¿™è¡Œï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
        }
        
        # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
        #self._print_final_report_v2(results)
        
        return results
    
    # 3. æ·»åŠ æ³¨æ„åŠ›éªŒè¯æ–¹æ³•
    def _validate_model_attention(self, model, test_data, model_name):
        """éªŒè¯æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒæ˜¯å¦å¹³è¡¡"""
        logger.info(f"\nğŸ” Validating attention distribution for {model_name}...")
        
        # éšæœºé€‰æ‹©5ä¸ªæ ·æœ¬è¿›è¡ŒéªŒè¯
        num_samples = min(5, len(test_data['bags']))
        sample_indices = np.random.choice(len(test_data['bags']), num_samples, replace=False)
        
        X_test = [
            test_data['bags'][sample_indices],
            test_data['instance_masks'][sample_indices],
            test_data['clinical_features'][sample_indices],
            test_data['side_masks'][sample_indices]
        ]
        
        # è·å–æ³¨æ„åŠ›è¾“å‡º
        outputs = model.attention_model.predict(X_test, verbose=0)
        predictions, left_attentions, right_attentions = outputs[:3]
        
        left_sums = []
        right_sums = []
        
        for i in range(num_samples):
            instance_mask = test_data['instance_masks'][sample_indices[i]]
            side_mask = test_data['side_masks'][sample_indices[i]]
            
            # è®¡ç®—å·¦å³ä¾§mask
            left_mask = instance_mask * (1 - side_mask)
            right_mask = instance_mask * side_mask
            
            # è·å–æœ‰æ•ˆçš„æ³¨æ„åŠ›æƒé‡
            left_valid_idx = np.where(left_mask > 0)[0]
            right_valid_idx = np.where(right_mask > 0)[0]
            
            if len(left_valid_idx) > 0:
                left_att_valid = left_attentions[i][left_valid_idx, 0]
                left_sum = np.sum(left_att_valid)
                left_sums.append(left_sum)
            
            if len(right_valid_idx) > 0:
                right_att_valid = right_attentions[i][right_valid_idx, 0]
                right_sum = np.sum(right_att_valid)
                right_sums.append(right_sum)
        
        # æŠ¥å‘Šç»“æœ
        if left_sums and right_sums:
            logger.info(f"   Left attention mean: {np.mean(left_sums):.3f} Â± {np.std(left_sums):.3f}")
            logger.info(f"   Right attention mean: {np.mean(right_sums):.3f} Â± {np.std(right_sums):.3f}")
            
            # æ£€æŸ¥æ˜¯å¦å¹³è¡¡
            if abs(np.mean(left_sums) - 1.0) < 0.1 and abs(np.mean(right_sums) - 1.0) < 0.1:
                logger.info("   âœ… Attention is well-balanced between left and right")
            else:
                logger.warning("   âš ï¸ Attention may be imbalanced")
    
    # 4. ä¿®æ”¹æœ€ç»ˆæŠ¥å‘Šæ–¹æ³•
    def _print_final_report_v2(self, results):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š - ç‹¬ç«‹æ³¨æ„åŠ›ç‰ˆæœ¬"""
        print("\n" + "="*80)
        print("ğŸ¯ BILATERAL MIL Model Selection - Final Report")
        print("ğŸ§  Using INDEPENDENT Attention Mechanisms")
        print("="*80)
        
        print("\nğŸ“Š Data Split:")
        print(f"   Train: {results['data_stats']['train_size']} patients")
        print(f"   Val: {results['data_stats']['val_size']} patients")
        print(f"   Test: {results['data_stats']['test_size']} patients")
        
        print("\nğŸ” Overfitting Analysis:")
        for model_name, info in results['overfitting_analysis'].items():
            status = "âš ï¸ OVERFITTING" if info['overfitting_detected'] else "âœ… OK"
            print(f"   {model_name}: {status}")
            print(f"      Train/Val gap: {info['accuracy_gap']:.3f}")
            print(f"      Best epoch: {info['best_epoch']+1}")
        
        print("\nğŸ“ˆ Test Set Performance Summary:")
        for result in results['all_test_results']:
            print(f"\n   {result['model_name']}:")
            print(f"      Accuracy: {result['accuracy']:.3f}")
            print(f"      High Risk Recall: {result['high_recall']:.3f}")
            print(f"      Medium Risk Recall: {result['medium_recall']:.3f}")
            print(f"      Threshold: {result['threshold']:.2f}")
        
        print(f"\nğŸ† SELECTED MODEL: {results['best_model']['model_name']}")
        print(f"   Selection Score: {results['best_model']['score']:.3f}")
        print(f"   âœ¨ Using independent attention for left and right breast")
        print(f"   âœ¨ Prevents attention bias and improves interpretability")
        
        print("="*80)

def validate_bilateral_model(model, data_manager, train_data, val_data, test_data, mil_data, output_dir):
    """ä¸“é—¨ä¸ºåŒä¾§æ¨¡å‹è®¾è®¡çš„éªŒè¯å‡½æ•°"""
    logger.info("ğŸ” å¼€å§‹åŒä¾§æ¨¡å‹æ€§èƒ½éªŒè¯...")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    X_test = [
        test_data['bags'],
        test_data['instance_masks'],
        test_data['clinical_features'],
        test_data['side_masks']  # åŒ…å«side_masks
    ]
    y_test = test_data['risk_labels']
    
    # è·å–é¢„æµ‹
    predictions = model.model.predict(X_test, verbose=0)
    pred_classes = np.argmax(predictions, axis=1)
    
    # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
    accuracy = accuracy_score(y_test, pred_classes)
    cm = confusion_matrix(y_test, pred_classes)
    report = classification_report(y_test, pred_classes, 
                                target_names=['Medium Risk', 'High Risk'],
                                output_dict=True)
    
    # æ ¼å¼åŒ–æ··æ·†çŸ©é˜µ
    cm_str = format_confusion_matrix(cm)
    
    # åˆ›å»ºéªŒè¯æŠ¥å‘Š
    validation_results = {
        'test_accuracy': accuracy,
        'confusion_matrix': cm,
        'classification_report': report,
        'predictions': predictions,
        'true_labels': y_test,
        'model_type': 'bilateral'
    }
    
    # æ‰“å°ç»“æœ
    logger.info("=" * 60)
    logger.info("ğŸ“Š åŒä¾§æ¨¡å‹éªŒè¯ç»“æœ")
    logger.info("=" * 60)
    logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.3f}")
    logger.info("æ··æ·†çŸ©é˜µ:")
    for line in cm_str.split('\n'):
        if line.strip():
            logger.info(line)
    
    logger.info("\nåˆ†ç±»æŠ¥å‘Š:")
    for class_name, metrics in report.items():
        if class_name in ['Medium Risk', 'High Risk']:
            logger.info(f"{class_name}:")
            logger.info(f"  Precision: {metrics['precision']:.3f}")
            logger.info(f"  Recall: {metrics['recall']:.3f}")
            logger.info(f"  F1-Score: {metrics['f1-score']:.3f}")
    
    # ä¿å­˜ç»“æœ
    results_file = os.path.join(output_dir, 'bilateral_validation_results.json')
    with open(results_file, 'w') as f:
        json.dump(safe_json_convert(validation_results), f, indent=2)
    logger.info(f"âœ… éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return validation_results, None

def main_bilateral_v2():
    """åŒä¾§ä¹³è…ºä¸»å‡½æ•° - ç‹¬ç«‹æ³¨æ„åŠ›ç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(description='Bilateral MIL Model with Independent Attention')
    parser.add_argument('--output-dir', type=str, default='D:/Desktop/bilateral_mil_output')
    parser.add_argument('--cache-root', type=str, default='./cache')
    parser.add_argument('--image-size', type=int, default=128)
    parser.add_argument('--max-instances', type=int, default=20)
    
    args = parser.parse_args()
    
    config = {
        'output_dir': args.output_dir,
        'cache_root': args.cache_root,
        'max_instances': args.max_instances,
        'image_config': {
            'target_size': (args.image_size, args.image_size)
        }
    }
    
    os.makedirs(config['output_dir'], exist_ok=True)
    
    setup_gpu()
    
    # è¿è¡ŒåŒä¾§è®­ç»ƒæµç¨‹ - ä½¿ç”¨ç‹¬ç«‹æ³¨æ„åŠ›
    pipeline = BilateralImprovedEnsembleMILPipeline(config)
    
    # ä½¿ç”¨æ–°çš„è®­ç»ƒæ–¹æ³•
    results = pipeline.run_bilateral_ensemble_training_v2()
    
    if results:
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆé¢„æµ‹
        best_model_name = results['best_model']['model_name']
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        best_model = None
        for model_info in pipeline.ensemble_models:
            if model_info['config']['name'] == best_model_name:
                best_model = model_info['model']
                break
        
        if best_model:
            # ä¿å­˜æ¨¡å‹æƒé‡
            weights_file = os.path.join(config['output_dir'], f'best_bilateral_independent_{best_model_name}.h5')
            best_model.model.save_weights(weights_file)
            
            # ========== æ–°å¢ï¼šç»¼åˆå¯è§†åŒ– ==========
            print("\n" + "ğŸ¨" * 40)
            print("å¼€å§‹ç”Ÿæˆç»¼åˆå­¦æœ¯å¯è§†åŒ–...")
            print("ğŸ¨" * 40)
            
            try:
                # å‡†å¤‡æ•°æ®
                mil_data = pipeline.data_manager.load_and_prepare_data()
                pipeline.data_manager.scaler.fit(mil_data['clinical_features'])
                mil_data['clinical_features'] = pipeline.data_manager.scaler.transform(mil_data['clinical_features'])
                train_data, val_data, test_data = pipeline._split_data_patient_aware(mil_data)
                
                # ========== ç”Ÿæˆ Grad-CAM å¯è§†åŒ– ==========
                print("\n" + "ğŸ”¥"*40)
                print("ç”Ÿæˆ Grad-CAM çƒ­åŠ›å›¾å¯è§†åŒ–...")
                print("ğŸ”¥"*40)
                
                try:
                    # ç”Ÿæˆ Grad-CAM å¯è§†åŒ–
                    gradcam_files = generate_improved_bilateral_gradcam(
                        best_model,
                        test_data,
                        config['output_dir']
                    )
                    
                    print("âœ… å¢å¼ºç‰ˆGradCAMå¯è§†åŒ–ç”ŸæˆæˆåŠŸï¼")
                    print("   - çƒ­å›¾å‡†ç¡®å åŠ åœ¨ä¹³è…ºç»„ç»‡ä¸Š")
                    print("   - è‡ªåŠ¨æ£€æµ‹å¹¶åˆ†ç¦»å·¦å³ä¹³è…º")
                    print("   - ä½¿ç”¨æ¤­åœ†å½¢é«˜æ–¯åˆ†å¸ƒåŒ¹é…ç»„ç»‡å½¢çŠ¶")
                    
                except Exception as e:
                    print(f"âŒ Grad-CAM ç”Ÿæˆå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()

                # 1. ç”ŸæˆåŸæœ‰çš„åŸºç¡€å¯è§†åŒ–ï¼ˆå¿«é€Ÿç‰ˆï¼‰
                from bilateral_attention_viz import visualize_bilateral_model_performance
                basic_viz_dir = visualize_bilateral_model_performance(
                    best_model,
                    test_data,
                    config['output_dir']
                )
                
                # 2. ç”Ÿæˆç»¼åˆå­¦æœ¯å¯è§†åŒ–ï¼ˆå®Œæ•´ç‰ˆï¼‰
                comprehensive_results = run_comprehensive_visualization(
                    best_model,
                    test_data,
                    config['output_dir'],
                    paper_style=True  # ä½¿ç”¨å­¦æœ¯è®ºæ–‡é£æ ¼
                )

                # ç”Ÿæˆä¸å¯¹ç§°æ€§åˆ†æ
                asymmetry_files = visualize_bilateral_asymmetry_learning(
                    best_model,
                    test_data,
                    config['output_dir']
                )
                
                print("\nâœ… æ‰€æœ‰å¯è§†åŒ–ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“ åŸºç¡€å¯è§†åŒ–ä¿å­˜åœ¨: {basic_viz_dir}")
                print(f"ğŸ“ å­¦æœ¯å¯è§†åŒ–ä¿å­˜åœ¨: {os.path.join(config['output_dir'], 'academic_visualizations')}")
                print("\nğŸ“„ æ‰“å¼€ä»¥ä¸‹æ–‡ä»¶æŸ¥çœ‹æ‰€æœ‰ç»“æœï¼š")
                print(f"   {os.path.join(config['output_dir'], 'academic_visualizations', 'visualization_index.html')}")
                
                # ========== ç”ŸæˆåŒä¾§ä¸å¯¹ç§°çƒ­å›¾å¯è§†åŒ– ==========
                print("\n" + "ğŸ”¥"*40)
                print("ç”ŸæˆåŒä¾§ä¸å¯¹ç§°æ€§çƒ­å›¾å¯è§†åŒ–...")
                print("ğŸ”¥"*40)

            except Exception as e:
                print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            print("âœ¨ ç”Ÿæˆçš„å¯è§†åŒ–åŒ…æ‹¬ï¼š")
            print("  - åŒä¾§æ³¨æ„åŠ›åˆ†æ")
            print("  - æ³¨æ„åŠ›çƒ­å›¾å åŠ ")
            print("  - æ³¨æ„åŠ›å¹³è¡¡æ€§åˆ†æ")
            print("  - ç»¼åˆæ€§èƒ½æŠ¥å‘Š")
    
    if results:
        return {
            'success': True,
            'best_model_name': results['best_model']['model_name'],
            'accuracy': results['best_model']['metrics']['accuracy']
        }
    else:
        return None


if __name__ == "__main__":
    result = main_bilateral_v2()
    if result and result.get('success'):
        sys.exit(0)
    else:
        sys.exit(1)