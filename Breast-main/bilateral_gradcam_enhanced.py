"""
åŒä¾§ä¹³è…ºGradCAMå¯è§†åŒ– - LDCTï¼ˆä½å‰‚é‡CTï¼‰ä¼˜åŒ–ç‰ˆæœ¬
é’ˆå¯¹ä¹³è…ºLDCTå›¾åƒçš„çª—å®½çª—ä½è¿›è¡Œä¼˜åŒ–
"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
from scipy.ndimage import gaussian_filter
import os
from matplotlib.gridspec import GridSpec
from tqdm import tqdm
import pydicom
import glob


class BilateralGradCAMLDCTOptimized:
    """é’ˆå¯¹LDCTä¼˜åŒ–çš„åŒä¾§GradCAMå¯è§†åŒ–å™¨"""
    
    def __init__(self, model, test_data, output_dir, dicom_root_dir=None):
        self.model = model
        self.test_data = test_data
        self.output_dir = output_dir
        self.gradcam_dir = os.path.join(output_dir, 'gradcam_ldct_optimized')
        os.makedirs(self.gradcam_dir, exist_ok=True)
        
        # DICOMæ ¹ç›®å½•
        self.dicom_root_dir = dicom_root_dir or 'D:/Desktop/Data_BI-RADS'
        
        # LDCTä¹³è…ºå›¾åƒçš„å…¸å‹çª—å®½çª—ä½è®¾ç½®
        self.breast_window = {
            'center': 50,    # è½¯ç»„ç»‡çª—ä½
            'width': 350     # è½¯ç»„ç»‡çª—å®½
        }
        
        # ç¼“å­˜åŸå§‹å›¾åƒ
        self.original_images_cache = {}
        
    def load_original_dicom_slices(self, patient_id):
        """åŠ è½½æ‚£è€…çš„åŸå§‹DICOMåˆ‡ç‰‡"""
        # å°è¯•ä¸åŒçš„è·¯å¾„æ ¼å¼
        possible_paths = [
            os.path.join(self.dicom_root_dir, patient_id),
            os.path.join(self.dicom_root_dir, f"P{patient_id}"),
            os.path.join(self.dicom_root_dir, f"Patient_{patient_id}"),
            os.path.join(self.dicom_root_dir, str(patient_id).zfill(6)),  # è¡¥é›¶æ ¼å¼
        ]
        
        patient_dir = None
        for path in possible_paths:
            if os.path.exists(path):
                patient_dir = path
                break
        
        if patient_dir is None:
            print(f"Warning: DICOM directory not found for patient {patient_id}")
            print(f"  Tried paths: {possible_paths}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰DICOMæ–‡ä»¶
        dicom_files = []
        for root, dirs, files in os.walk(patient_dir):
            for file in files:
                if file.endswith('.dcm'):
                    dicom_files.append(os.path.join(root, file))
        
        if not dicom_files:
            print(f"Warning: No DICOM files found for patient {patient_id}")
            return None
        
        # åŠ è½½å’Œæ’åºDICOMåˆ‡ç‰‡
        slices = []
        for file_path in dicom_files:
            try:
                ds = pydicom.dcmread(file_path)
                if hasattr(ds, 'pixel_array'):
                    slices.append(ds)
            except Exception as e:
                print(f"Error reading DICOM file {file_path}: {e}")
                continue
        
        # æŒ‰ç…§åˆ‡ç‰‡ä½ç½®æ’åº
        slices.sort(key=lambda x: float(x.ImagePositionPatient[2]) if hasattr(x, 'ImagePositionPatient') else 0)
        
        # æå–å›¾åƒæ•°ç»„å¹¶åº”ç”¨é€‚åˆä¹³è…ºLDCTçš„çª—å£è®¾ç½®
        image_arrays = []
        for ds in slices:
            img = ds.pixel_array.astype(np.float32)
            
            # åº”ç”¨Rescale Slopeå’ŒInterceptè½¬æ¢åˆ°HUå€¼
            if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):
                img = img * ds.RescaleSlope + ds.RescaleIntercept
            
            # åº”ç”¨ä¹³è…ºè½¯ç»„ç»‡çª—å£
            window_center = self.breast_window['center']
            window_width = self.breast_window['width']
            
            # å¦‚æœDICOMæ–‡ä»¶ä¸­æœ‰ç‰¹å®šçš„çª—å£è®¾ç½®ï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨
            if hasattr(ds, 'WindowCenter') and hasattr(ds, 'WindowWidth'):
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªçª—å£ï¼ˆæŸäº›DICOMå¯èƒ½æœ‰å¤šä¸ªé¢„è®¾çª—å£ï¼‰
                if isinstance(ds.WindowCenter, list):
                    # å¯»æ‰¾è½¯ç»„ç»‡çª—å£ï¼ˆé€šå¸¸window centeråœ¨0-100ä¹‹é—´ï¼‰
                    for i, wc in enumerate(ds.WindowCenter):
                        if 0 <= wc <= 100:
                            window_center = float(wc)
                            window_width = float(ds.WindowWidth[i])
                            break
                else:
                    # åªæœ‰å½“çª—ä½åœ¨åˆç†èŒƒå›´å†…æ‰ä½¿ç”¨DICOMçš„è®¾ç½®
                    try:
                        wc_value = float(ds.WindowCenter)
                        ww_value = float(ds.WindowWidth)
                        if -100 <= wc_value <= 200:
                            window_center = wc_value
                            window_width = ww_value
                    except (TypeError, ValueError):
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        pass
            
            print(f"  Using window: C={window_center}, W={window_width}")
            
            # åº”ç”¨çª—å£
            img_min = window_center - window_width / 2
            img_max = window_center + window_width / 2
            img_windowed = np.clip(img, img_min, img_max)
            
            # å½’ä¸€åŒ–åˆ°0-1
            img_normalized = (img_windowed - img_min) / (img_max - img_min)
            
            image_arrays.append(img_normalized)
        
        return image_arrays
    
    def enhance_ldct_for_display(self, ct_image):
        """é’ˆå¯¹LDCTå›¾åƒçš„å¢å¼ºå¤„ç†"""
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        img = np.array(ct_image)
        
        # å¦‚æœå·²ç»æ˜¯0-255èŒƒå›´ï¼Œè½¬æ¢åˆ°0-1
        if img.max() > 1:
            img = img / 255.0
        
        # å¤„ç†å¤šé€šé“å›¾åƒ
        if len(img.shape) == 3:
            # å¦‚æœæ˜¯RGBä¸”ä¸‰ä¸ªé€šé“ç›¸åŒï¼Œè½¬æ¢ä¸ºç°åº¦
            if img.shape[2] == 3 and np.allclose(img[:,:,0], img[:,:,1]) and np.allclose(img[:,:,1], img[:,:,2]):
                img = img[:,:,0]
            else:
                # è½¬æ¢ä¸ºç°åº¦
                img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY) / 255.0
        
        # è½¬æ¢åˆ°uint8
        img_uint8 = (img * 255).astype(np.uint8)
        
        # ç¡®ä¿æ˜¯å•é€šé“
        if len(img_uint8.shape) > 2:
            img_uint8 = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2GRAY)
        
        # 1. è½»å¾®çš„å¯¹æ¯”åº¦å¢å¼ºï¼ˆLDCTä¸éœ€è¦å¤ªå¼ºçš„å¢å¼ºï¼‰
        clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8,8))  # é™ä½clipLimit
        enhanced = clahe.apply(img_uint8)
        
        # 2. åº”ç”¨è½»å¾®çš„ä¼½é©¬æ ¡æ­£æ”¹å–„è½¯ç»„ç»‡å¯¹æ¯”åº¦
        gamma = 0.9  # è½»å¾®æäº®
        inv_gamma = 1.0 / gamma
        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
        enhanced = cv2.LUT(enhanced, table)
        
        # 3. è½»å¾®çš„å»å™ªï¼ˆLDCTå›¾åƒé€šå¸¸å™ªå£°è¾ƒå¤šï¼‰
        enhanced = cv2.bilateralFilter(enhanced, 5, 30, 30)
        
        # è½¬æ¢ä¸ºRGB
        enhanced_rgb = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2RGB)
        
        return enhanced_rgb
    
    def resize_to_match(self, original_img, target_shape):
        """è°ƒæ•´åŸå§‹å›¾åƒå¤§å°ä»¥åŒ¹é…ç›®æ ‡å½¢çŠ¶"""
        target_h, target_w = target_shape[:2]
        
        # ä½¿ç”¨é«˜è´¨é‡çš„æ’å€¼æ–¹æ³•
        resized = cv2.resize(original_img, (target_w, target_h), interpolation=cv2.INTER_CUBIC)
        
        return resized
    
    def find_matching_slice(self, processed_slice, original_slices):
        """é€šè¿‡å›¾åƒç›¸ä¼¼åº¦æ‰¾åˆ°åŒ¹é…çš„åŸå§‹åˆ‡ç‰‡"""
        if not original_slices:
            return None
        
        # å°†å¤„ç†åçš„åˆ‡ç‰‡è½¬æ¢ä¸ºç°åº¦
        if len(processed_slice.shape) == 3:
            processed_gray = cv2.cvtColor((processed_slice * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
        else:
            processed_gray = (processed_slice * 255).astype(np.uint8)
        
        # å¯»æ‰¾æœ€åŒ¹é…çš„åˆ‡ç‰‡
        best_match_idx = len(original_slices) // 2  # é»˜è®¤ä½¿ç”¨ä¸­é—´åˆ‡ç‰‡
        best_score = -1
        
        # åªæ£€æŸ¥ä¸­é—´60%çš„åˆ‡ç‰‡ï¼ˆé€šå¸¸ä¹³è…ºç»„ç»‡åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼‰
        start_idx = int(len(original_slices) * 0.2)
        end_idx = int(len(original_slices) * 0.8)
        
        for i in range(start_idx, end_idx):
            # è°ƒæ•´åŸå§‹åˆ‡ç‰‡å¤§å°ä»¥åŒ¹é…
            original_resized = self.resize_to_match(original_slices[i], processed_slice.shape)
            original_gray = (original_resized * 255).astype(np.uint8)
            if len(original_gray.shape) == 3:
                original_gray = cv2.cvtColor(original_gray, cv2.COLOR_RGB2GRAY)
            
            # è®¡ç®—ç»“æ„ç›¸ä¼¼æ€§
            score = cv2.matchTemplate(processed_gray, original_gray, cv2.TM_CCOEFF_NORMED)
            
            if score.max() > best_score:
                best_score = score.max()
                best_match_idx = i
        
        return original_slices[best_match_idx]
    
    def generate_attention_heatmap_on_tissue(self, slice_img, attention_weight):
        """åœ¨ç»„ç»‡åŒºåŸŸç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾"""
        h, w = slice_img.shape[:2]
        
        # æ£€æµ‹éé›¶åŒºåŸŸï¼ˆç»„ç»‡åŒºåŸŸï¼‰
        if len(slice_img.shape) == 3:
            gray = cv2.cvtColor((slice_img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)
        else:
            gray = (slice_img * 255).astype(np.uint8)
        
        # æ‰¾åˆ°ç»„ç»‡åŒºåŸŸ
        _, tissue_mask = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)
        
        # åˆ›å»ºçƒ­å›¾
        heatmap = np.zeros((h, w), dtype=np.float32)
        
        if attention_weight > 0.01 and np.sum(tissue_mask) > 0:
            # æ‰¾åˆ°ç»„ç»‡çš„è´¨å¿ƒ
            contours, _ = cv2.findContours(tissue_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            if contours:
                largest_contour = max(contours, key=cv2.contourArea)
                M = cv2.moments(largest_contour)
                
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                else:
                    cx, cy = w // 2, h // 2
                
                # è·å–è¾¹ç•Œæ¡†
                x, y, bbox_w, bbox_h = cv2.boundingRect(largest_contour)
                
                # åˆ›å»ºé«˜æ–¯åˆ†å¸ƒçš„çƒ­å›¾
                Y, X = np.ogrid[:h, :w]
                sigma_x = bbox_w / 4
                sigma_y = bbox_h / 4
                gaussian = np.exp(-((X - cx)**2 / (2 * sigma_x**2) + 
                                   (Y - cy)**2 / (2 * sigma_y**2)))
                
                # åªåœ¨ç»„ç»‡åŒºåŸŸå†…æ˜¾ç¤ºçƒ­å›¾
                heatmap = gaussian * (tissue_mask / 255.0) * attention_weight
                
                # è½»å¾®æ¨¡ç³Š
                heatmap = gaussian_filter(heatmap, sigma=2)
        
        # å½’ä¸€åŒ–
        if heatmap.max() > 0:
            heatmap = heatmap / heatmap.max()
        
        return heatmap
    
    def create_overlay_with_original_ct(self, processed_slice, heatmap, original_ct, alpha=0.35):
        """åœ¨åŸå§‹CTå›¾åƒä¸Šå åŠ çƒ­å›¾"""
        # å¢å¼ºåŸå§‹LDCTå›¾åƒ
        enhanced_ct = self.enhance_ldct_for_display(original_ct)
        
        # è°ƒæ•´å¤§å°ä»¥åŒ¹é…å¤„ç†åçš„åˆ‡ç‰‡
        if enhanced_ct.shape[:2] != processed_slice.shape[:2]:
            enhanced_ct = self.resize_to_match(enhanced_ct, processed_slice.shape)
        
        # ç¡®ä¿æ˜¯RGBæ ¼å¼
        if len(enhanced_ct.shape) == 2:
            enhanced_ct = cv2.cvtColor(enhanced_ct, cv2.COLOR_GRAY2RGB)
        
        # åˆ›å»ºçƒ­å›¾çš„å½©è‰²ç‰ˆæœ¬
        if heatmap.max() > 0:
            heatmap_uint8 = (heatmap * 255).astype(np.uint8)
            heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
            
            # åˆ›å»ºalphaé€šé“
            heatmap_alpha = (heatmap > 0.1).astype(np.float32)
            heatmap_alpha = cv2.GaussianBlur(heatmap_alpha, (5, 5), 0)
            adaptive_alpha = alpha * heatmap_alpha[:, :, np.newaxis]
            
            # å åŠ 
            overlay = enhanced_ct.astype(np.float32)
            overlay = overlay * (1 - adaptive_alpha) + heatmap_colored.astype(np.float32) * adaptive_alpha
            overlay = np.clip(overlay, 0, 255).astype(np.uint8)
        else:
            overlay = enhanced_ct
        
        return overlay
    
    def extract_attention_and_features(self, sample_idx):
        """æå–æ³¨æ„åŠ›æƒé‡å’Œç‰¹å¾"""
        X_sample = [
            self.test_data['bags'][sample_idx:sample_idx+1],
            self.test_data['instance_masks'][sample_idx:sample_idx+1],
            self.test_data['clinical_features'][sample_idx:sample_idx+1],
            self.test_data['side_masks'][sample_idx:sample_idx+1]
        ]
        
        # è·å–æ³¨æ„åŠ›è¾“å‡º
        outputs = self.model.attention_model.predict(X_sample, verbose=0)
        prediction, left_attention, right_attention = outputs[:3]
        
        instance_mask = self.test_data['instance_masks'][sample_idx]
        side_mask = self.test_data['side_masks'][sample_idx]
        
        # åˆ†ç¦»å·¦å³ä¾§
        left_mask = instance_mask * (1 - side_mask)
        right_mask = instance_mask * side_mask
        
        left_indices = np.where(left_mask > 0)[0]
        right_indices = np.where(right_mask > 0)[0]
        
        # æå–æ³¨æ„åŠ›æƒé‡
        left_weights = {}
        right_weights = {}
        
        if len(left_indices) > 0:
            for i, idx in enumerate(left_indices):
                if idx < len(left_attention[0]):
                    left_weights[idx] = float(left_attention[0, idx, 0])
        
        if len(right_indices) > 0:
            for i, idx in enumerate(right_indices):
                if idx < len(right_attention[0]):
                    right_weights[idx] = float(right_attention[0, idx, 0])
        
        return {
            'prediction': prediction[0],
            'left_indices': left_indices,
            'right_indices': right_indices,
            'left_weights': left_weights,
            'right_weights': right_weights,
            'true_label': self.test_data['risk_labels'][sample_idx]
        }
    
    def visualize_bilateral_gradcam_ldct(self, sample_idx):
        """åˆ›å»ºé’ˆå¯¹LDCTä¼˜åŒ–çš„åŒä¾§GradCAMå¯è§†åŒ–"""
        # è·å–æ•°æ®
        attention_data = self.extract_attention_and_features(sample_idx)
        bag = self.test_data['bags'][sample_idx]
        bag_info = self.test_data['bag_info'][sample_idx] if sample_idx < len(self.test_data['bag_info']) else {}
        
        # è·å–æ‚£è€…ID
        patient_id = bag_info.get('patient_id', f'Sample_{sample_idx}')
        
        # åŠ è½½åŸå§‹DICOMå›¾åƒ
        print(f"\nLoading original LDCT images for patient {patient_id}...")
        original_slices = self.load_original_dicom_slices(str(patient_id))
        
        if original_slices is None:
            print(f"Warning: Could not load original images for patient {patient_id}")
            use_original = False
        else:
            print(f"Loaded {len(original_slices)} original LDCT slices")
            use_original = True
        
        # åˆ›å»ºå›¾å½¢
        fig = plt.figure(figsize=(20, 12))
        gs = GridSpec(3, 4, figure=fig, hspace=0.3, wspace=0.3)
        
        # è®¾ç½®æ·±è‰²èƒŒæ™¯
        fig.patch.set_facecolor('#f0f0f0')
        
        # æ ‡é¢˜
        pred_prob = attention_data['prediction']
        pred_label = 1 if pred_prob[1] > 0.5 else 0
        true_label = attention_data['true_label']
        
        title = f'Bilateral GradCAM on LDCT - Patient {patient_id}\n'
        title += f'True: {"High Risk" if true_label else "Medium Risk"} | '
        title += f'Predicted: {"High Risk" if pred_label else "Medium Risk"} '
        title += f'(Prob: {pred_prob[1]:.3f})'
        
        if use_original:
            title += '\n[Original LDCT with Optimized Window]'
        else:
            title += '\n[Processed Images Only]'
        
        fig.suptitle(title, fontsize=16, fontweight='bold')
        
        # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ‡ç‰‡
        display_slices = []
        
        # æ‰¾åˆ°å·¦å³ä¾§æœ€é«˜æ³¨æ„åŠ›åˆ‡ç‰‡
        left_max_idx = None
        left_max_weight = 0
        for idx, weight in attention_data['left_weights'].items():
            if weight > left_max_weight:
                left_max_weight = weight
                left_max_idx = idx
        
        right_max_idx = None
        right_max_weight = 0
        for idx, weight in attention_data['right_weights'].items():
            if weight > right_max_weight:
                right_max_weight = weight
                right_max_idx = idx
        
        if left_max_idx is not None:
            display_slices.append(('Left Max', left_max_idx, 'left'))
        if right_max_idx is not None:
            display_slices.append(('Right Max', right_max_idx, 'right'))
        
        # æ·»åŠ å…¶ä»–é«˜æ³¨æ„åŠ›åˆ‡ç‰‡
        all_weights = list(attention_data['left_weights'].items()) + list(attention_data['right_weights'].items())
        sorted_weights = sorted(all_weights, key=lambda x: x[1], reverse=True)
        
        for idx, weight in sorted_weights[:12]:
            if idx in attention_data['left_indices']:
                side = 'left'
            else:
                side = 'right'
            
            if (idx, side) not in [(s[1], s[2]) for s in display_slices]:
                display_slices.append((f'{side.title()} #{idx}', idx, side))
            
            if len(display_slices) >= 12:
                break
        
        # ä¸ºæ¯ä¸ªåˆ‡ç‰‡åˆ›å»ºå¯è§†åŒ–
        for i, (label, slice_idx, side) in enumerate(display_slices[:12]):
            if i >= 12 or slice_idx >= len(bag):
                continue
            
            row = i // 4
            col = i % 4
            ax = fig.add_subplot(gs[row, col])
            
            # è·å–å¤„ç†åçš„åˆ‡ç‰‡
            processed_slice = bag[slice_idx]
            
            # è·å–æ³¨æ„åŠ›æƒé‡
            if side == 'left':
                weight = attention_data['left_weights'].get(slice_idx, 0)
            else:
                weight = attention_data['right_weights'].get(slice_idx, 0)
            
            # ç”Ÿæˆçƒ­å›¾
            heatmap = self.generate_attention_heatmap_on_tissue(processed_slice, weight)
            
            # åˆ›å»ºå åŠ å›¾
            if use_original and original_slices:
                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…æ–¹æ³•
                original_ct = self.find_matching_slice(processed_slice, original_slices)
                if original_ct is not None:
                    overlay = self.create_overlay_with_original_ct(processed_slice, heatmap, original_ct)
                else:
                    # å¦‚æœåŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨ç´¢å¼•åŒ¹é…
                    original_idx = min(slice_idx, len(original_slices) - 1)
                    original_ct = original_slices[original_idx]
                    overlay = self.create_overlay_with_original_ct(processed_slice, heatmap, original_ct)
            else:
                # ä½¿ç”¨å¤„ç†åçš„å›¾åƒ
                overlay = self.create_overlay_with_original_ct(processed_slice, heatmap, processed_slice)
            
            # æ˜¾ç¤º
            ax.imshow(overlay, cmap='gray' if len(overlay.shape) == 2 else None)
            ax.set_title(f'{label} (Slice #{slice_idx})\n'
                        f'Attention: {weight:.3f}',
                        fontsize=10)
            ax.axis('off')
            
            # æ ¹æ®æ³¨æ„åŠ›å¼ºåº¦æ·»åŠ è¾¹æ¡†
            if weight > 0.1:
                edge_color = 'gold'
                edge_width = 3
            elif weight > 0.05:
                edge_color = 'orange'
                edge_width = 2
            else:
                edge_color = 'silver'
                edge_width = 1
                
            for spine in ax.spines.values():
                spine.set_edgecolor(edge_color)
                spine.set_linewidth(edge_width)
            
            # æ·»åŠ ä¾§åˆ«æ ‡è®°
            ax.text(0.05, 0.95, side.upper()[0], transform=ax.transAxes,
                   fontsize=14, fontweight='bold', color='white',
                   bbox=dict(boxstyle='round,pad=0.3', 
                           facecolor='red' if side == 'right' else 'blue', 
                           alpha=0.7))
        
        # ä¿å­˜
        save_path = os.path.join(self.gradcam_dir, f'bilateral_gradcam_ldct_{sample_idx}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='#f0f0f0')
        plt.close()
        
        return save_path
    
    def generate_all_visualizations(self, num_samples=5):
        """ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–"""
        print("\nğŸ¨ Generating Bilateral GradCAM with LDCT Optimization...")
        print(f"   DICOM directory: {self.dicom_root_dir}")
        print(f"   Using breast tissue window: C={self.breast_window['center']}, W={self.breast_window['width']}")
        
        sample_indices = np.random.choice(
            len(self.test_data['bags']), 
            min(num_samples, len(self.test_data['bags'])), 
            replace=False
        )
        
        generated_files = []
        
        for i, sample_idx in enumerate(sample_indices):
            print(f"\n  Processing sample {i+1}/{len(sample_indices)}...")
            try:
                file_path = self.visualize_bilateral_gradcam_ldct(sample_idx)
                generated_files.append(file_path)
                print(f"    âœ… Saved: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"    âŒ Failed: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\nâœ… LDCT-optimized GradCAM visualization completed!")
        print(f"ğŸ“ Generated {len(generated_files)} files")
        print(f"ğŸ“‚ Saved to: {self.gradcam_dir}")
        
        return generated_files


def generate_improved_bilateral_gradcam(model, test_data, output_dir, dicom_root_dir='D:/Desktop/Data_BI-RADS'):
    """ç”Ÿæˆé’ˆå¯¹LDCTä¼˜åŒ–çš„åŒä¾§GradCAMå¯è§†åŒ–"""
    print("\n" + "ğŸ”¥"*30)
    print("Generating Bilateral GradCAM with LDCT Optimization")
    print("ğŸ”¥"*30)
    
    visualizer = BilateralGradCAMLDCTOptimized(
        model, 
        test_data, 
        output_dir,
        dicom_root_dir=dicom_root_dir
    )
    
    generated_files = visualizer.generate_all_visualizations(num_samples=5)
    
    print("\nâœ… LDCT-optimized visualization complete!")
    print("   âœ¨ Soft tissue window applied (C=50, W=350)")
    print("   âœ¨ Enhanced contrast for breast tissue")
    print("   âœ¨ Reduced noise for better clarity")
    print("   âœ¨ Attention heatmaps on anatomical structures")
    
    return generated_files